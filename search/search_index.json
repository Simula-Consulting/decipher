{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Decipher Decipher is a collection of tools produced by Simula Consulting as an end product from the Decipher project. Warning Decipher is currently under development. Major changes in design and functioality is to be expected! Ownership The code product is originally written by Severin Langberg! Overview Decipher currently consist of two main models MatFact HMM (Hidden Markov Model) The goal of Decipher is to offer a simple unified API to all models, for training and testing. # Ideal future usage from matfact import matfact from hmm_synthetic import hmm for model in ( matfact , hmm ): # (1)! model () . fit ( some_data ) Right now, fit of HMM is not implemented. MLFlow logging We use MLFlow for tracking experiments. See their tracking docs for details. However, we also have added some convenience wrappers around MLFlow. Instead of mlflow.start_run , use MLFlowLogger . Visualization We have also made a simple pilot for interactive data visualization. Discussion of models The purpose of the project is to personalize cerivcal cancer screening. There are however many approaches to this, and models for several different tasks have been developed. The datasets First, let us mention the data we have available: Data screening results Discrete time series of measurement results from screening exams. The time series is /very/ sparse, with one measurement about every three years. The measurement result is one of four severity levels: 1. Normal 2. Low-risk 3. High-risk 4. Cancer We typically represent the time series as a \\((N \\times T)\\) array, where \\(N\\) is the number of individuals and \\(T\\) the number of time steps. Info The majority of the code developed so far, is solely concerned with this type of data. Details on the four risk levels The clinical exam results in one of many (> 10 ) diagnoses, several of which are overlapping in severity. These are then mapped onto one of the four severity levels. The number four was chosen as a compromise between having sufficient resolution while still getting enough samples from each state. Some discussion on this can be found in the HMM paper on page 6. HPV data Some individuals take an HPV (Human papillomavirus) test. For this data, we have registered when the test was taken, and its result (if and possibly which virus subtypes were found). This data has been used in conjuction with the screening data to learn the relative importance of the different virus types as a risk predictor. Lifestyle questionnaire Questionnaire data on lifestyle, such as number of sexual partners, age at first sexual intercourse, smoking habits, drinking habits, etc. The models The models that have been developed/investigated is MatFact Trained on screening data Can predict the state of an unseen history at a given time point HMM (Hidden Markov Model) Trained on screening data Can: Synthezise screening data Predict the state for an unseen history at a given time point Fit not implemented The fitting of the HMM is not implemented Hidden and observed layers The current HMM implementation only implements the hidden variables. The observation layer is not implemented, and the outputted states are simply the hidden states. HPV survival model Trained on HPV and screening data. Can give weights for the relative risk level of the different HPV types GDL (geometric deep learning)","title":"Overview"},{"location":"#welcome-to-decipher","text":"Decipher is a collection of tools produced by Simula Consulting as an end product from the Decipher project. Warning Decipher is currently under development. Major changes in design and functioality is to be expected! Ownership The code product is originally written by Severin Langberg!","title":"Welcome to Decipher"},{"location":"#overview","text":"Decipher currently consist of two main models MatFact HMM (Hidden Markov Model) The goal of Decipher is to offer a simple unified API to all models, for training and testing. # Ideal future usage from matfact import matfact from hmm_synthetic import hmm for model in ( matfact , hmm ): # (1)! model () . fit ( some_data ) Right now, fit of HMM is not implemented.","title":"Overview"},{"location":"#mlflow-logging","text":"We use MLFlow for tracking experiments. See their tracking docs for details. However, we also have added some convenience wrappers around MLFlow. Instead of mlflow.start_run , use MLFlowLogger .","title":"MLFlow logging"},{"location":"#visualization","text":"We have also made a simple pilot for interactive data visualization.","title":"Visualization"},{"location":"#discussion-of-models","text":"The purpose of the project is to personalize cerivcal cancer screening. There are however many approaches to this, and models for several different tasks have been developed.","title":"Discussion of models"},{"location":"#the-datasets","text":"First, let us mention the data we have available: Data screening results Discrete time series of measurement results from screening exams. The time series is /very/ sparse, with one measurement about every three years. The measurement result is one of four severity levels: 1. Normal 2. Low-risk 3. High-risk 4. Cancer We typically represent the time series as a \\((N \\times T)\\) array, where \\(N\\) is the number of individuals and \\(T\\) the number of time steps. Info The majority of the code developed so far, is solely concerned with this type of data. Details on the four risk levels The clinical exam results in one of many (> 10 ) diagnoses, several of which are overlapping in severity. These are then mapped onto one of the four severity levels. The number four was chosen as a compromise between having sufficient resolution while still getting enough samples from each state. Some discussion on this can be found in the HMM paper on page 6. HPV data Some individuals take an HPV (Human papillomavirus) test. For this data, we have registered when the test was taken, and its result (if and possibly which virus subtypes were found). This data has been used in conjuction with the screening data to learn the relative importance of the different virus types as a risk predictor. Lifestyle questionnaire Questionnaire data on lifestyle, such as number of sexual partners, age at first sexual intercourse, smoking habits, drinking habits, etc.","title":"The datasets"},{"location":"#the-models","text":"The models that have been developed/investigated is MatFact Trained on screening data Can predict the state of an unseen history at a given time point HMM (Hidden Markov Model) Trained on screening data Can: Synthezise screening data Predict the state for an unseen history at a given time point Fit not implemented The fitting of the HMM is not implemented Hidden and observed layers The current HMM implementation only implements the hidden variables. The observation layer is not implemented, and the outputted states are simply the hidden states. HPV survival model Trained on HPV and screening data. Can give weights for the relative risk level of the different HPV types GDL (geometric deep learning)","title":"The models"},{"location":"hmm/","text":"HMM - Hidden Markov Model exit_time ( u_random_variable , age , state , age_partition_index , l_partition_index , time_grid ) Random exit time from current state. Notation used - age_partition_index: corresponds to k in the paper - l_partition_index: corresponds to l in the paper. It is the partition index of the exit age (ish) Source code in hmm_synthetic/backend/sojourn.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def exit_time ( u_random_variable : float , age : int , state : int , age_partition_index : int , l_partition_index : int , time_grid : Sequence [ int ] | npt . NDArray [ np . int_ ], ) -> float : \"\"\"Random exit time from current state. Notation used - age_partition_index: corresponds to k in the paper - l_partition_index: corresponds to l in the paper. It is the partition index of the exit age (ish) \"\"\" sum_kappa = sum ( [ kappa ( age , state , i , time_grid , l_partition_index = l_partition_index ) for i in range ( 1 , l_partition_index - age_partition_index + 1 ) ] ) return ( sum_kappa - np . log ( 1 - u_random_variable )) / sum ( legal_transition_lambdas ( state , l_partition_index ) ) search_l ( u_random_variable , age_partition_index , current_age_pts , state , time_grid , n_age_partitions = 8 ) Find the partition satisfying the probability requirement. Find l such that \\[ P[T_s(a) < tau_l - age] < u < P[T_s(a) < tau_{l+1} - q], \\] where \\[ P[T_s(a) > t] = exp( kappa_0^s + kappa_1^s + sum_{i=2}^n kappa_i^s ) = exp( sum_i kappa_i^s) \\] In other words, find the largest l such that [ P[T_s(a) < tau_l - a] < u. ] Source code in hmm_synthetic/backend/sojourn.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def search_l ( u_random_variable : float , age_partition_index : int , current_age_pts : int , state : int , time_grid : Sequence [ int ] | npt . NDArray [ np . int_ ], n_age_partitions : int = 8 , ) -> int : r \"\"\"Find the partition satisfying the probability requirement. Find l such that $$ P[T_s(a) < tau_l - age] < u < P[T_s(a) < tau_{l+1} - q], $$ where $$ P[T_s(a) > t] = exp( kappa_0^s + kappa_1^s + sum_{i=2}^n kappa_i^s ) = exp( sum_i kappa_i^s) $$ In other words, find the largest l such that \\[ P[T_s(a) < tau_l - a] < u. \\] \"\"\" if age_partition_index == n_age_partitions - 1 : return age_partition_index for l_partition_index_candidate in range ( age_partition_index , n_age_partitions ): partition_upper_limit = time_grid [ l_partition_index_candidate + 1 ] t = partition_upper_limit - current_age_pts # Evaluate the sojourn time CDF at this time step. cdf = 1.0 - np . exp ( sum ( [ kappa ( current_age_pts , state , i , time_grid , t = t ) for i in range ( l_partition_index_candidate - age_partition_index + 1 ) ] ) ) if cdf > u_random_variable : return l_partition_index_candidate - 1 return l_partition_index_candidate - 1 time_exit_state ( current_age_pts , age_max_pts , state , time_grid , rnd ) Returns the amount of time a female spends in the current state. Source code in hmm_synthetic/backend/sojourn.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def time_exit_state ( current_age_pts : int , age_max_pts : int , state : int , time_grid : Sequence [ int ] | npt . NDArray [ np . int_ ], rnd : np . random . Generator , ) -> float : \"\"\"Returns the amount of time a female spends in the current state.\"\"\" # Need t > 0. # TODO: Is this line correct, or does it assume ints? # exit_time can output a number x s.t. 0 < x < 1. # Should the test instead be <= 0? if abs ( age_max_pts - current_age_pts ) <= 1 : return age_max_pts # Corollary 1: Steps 1-4 u = rnd . uniform () k = age_group_idx ( current_age_pts , time_grid ) l = search_l ( u , k , current_age_pts , state , time_grid ) # noqa: E741 return exit_time ( u , current_age_pts , state , k , l , time_grid )","title":"HMM"},{"location":"hmm/#hmm-hidden-markov-model","text":"","title":"HMM - Hidden Markov Model"},{"location":"hmm/#hmm_synthetic.backend.sojourn.exit_time","text":"Random exit time from current state. Notation used - age_partition_index: corresponds to k in the paper - l_partition_index: corresponds to l in the paper. It is the partition index of the exit age (ish) Source code in hmm_synthetic/backend/sojourn.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def exit_time ( u_random_variable : float , age : int , state : int , age_partition_index : int , l_partition_index : int , time_grid : Sequence [ int ] | npt . NDArray [ np . int_ ], ) -> float : \"\"\"Random exit time from current state. Notation used - age_partition_index: corresponds to k in the paper - l_partition_index: corresponds to l in the paper. It is the partition index of the exit age (ish) \"\"\" sum_kappa = sum ( [ kappa ( age , state , i , time_grid , l_partition_index = l_partition_index ) for i in range ( 1 , l_partition_index - age_partition_index + 1 ) ] ) return ( sum_kappa - np . log ( 1 - u_random_variable )) / sum ( legal_transition_lambdas ( state , l_partition_index ) )","title":"exit_time()"},{"location":"hmm/#hmm_synthetic.backend.sojourn.search_l","text":"Find the partition satisfying the probability requirement. Find l such that \\[ P[T_s(a) < tau_l - age] < u < P[T_s(a) < tau_{l+1} - q], \\] where \\[ P[T_s(a) > t] = exp( kappa_0^s + kappa_1^s + sum_{i=2}^n kappa_i^s ) = exp( sum_i kappa_i^s) \\] In other words, find the largest l such that [ P[T_s(a) < tau_l - a] < u. ] Source code in hmm_synthetic/backend/sojourn.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def search_l ( u_random_variable : float , age_partition_index : int , current_age_pts : int , state : int , time_grid : Sequence [ int ] | npt . NDArray [ np . int_ ], n_age_partitions : int = 8 , ) -> int : r \"\"\"Find the partition satisfying the probability requirement. Find l such that $$ P[T_s(a) < tau_l - age] < u < P[T_s(a) < tau_{l+1} - q], $$ where $$ P[T_s(a) > t] = exp( kappa_0^s + kappa_1^s + sum_{i=2}^n kappa_i^s ) = exp( sum_i kappa_i^s) $$ In other words, find the largest l such that \\[ P[T_s(a) < tau_l - a] < u. \\] \"\"\" if age_partition_index == n_age_partitions - 1 : return age_partition_index for l_partition_index_candidate in range ( age_partition_index , n_age_partitions ): partition_upper_limit = time_grid [ l_partition_index_candidate + 1 ] t = partition_upper_limit - current_age_pts # Evaluate the sojourn time CDF at this time step. cdf = 1.0 - np . exp ( sum ( [ kappa ( current_age_pts , state , i , time_grid , t = t ) for i in range ( l_partition_index_candidate - age_partition_index + 1 ) ] ) ) if cdf > u_random_variable : return l_partition_index_candidate - 1 return l_partition_index_candidate - 1","title":"search_l()"},{"location":"hmm/#hmm_synthetic.backend.sojourn.time_exit_state","text":"Returns the amount of time a female spends in the current state. Source code in hmm_synthetic/backend/sojourn.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def time_exit_state ( current_age_pts : int , age_max_pts : int , state : int , time_grid : Sequence [ int ] | npt . NDArray [ np . int_ ], rnd : np . random . Generator , ) -> float : \"\"\"Returns the amount of time a female spends in the current state.\"\"\" # Need t > 0. # TODO: Is this line correct, or does it assume ints? # exit_time can output a number x s.t. 0 < x < 1. # Should the test instead be <= 0? if abs ( age_max_pts - current_age_pts ) <= 1 : return age_max_pts # Corollary 1: Steps 1-4 u = rnd . uniform () k = age_group_idx ( current_age_pts , time_grid ) l = search_l ( u , k , current_age_pts , state , time_grid ) # noqa: E741 return exit_time ( u , current_age_pts , state , k , l , time_grid )","title":"time_exit_state()"},{"location":"matfact/","text":"MatFact Matrix Factorization with temporal regularization. Example from matfact.model import train_and_log from matfact.data_generation import Dataset data = Dataset . generate () X_train , X_test , _ , _ = data . get_split_X_M () train_and_log ( X_train , X_test ) matfact.model.matfact ArgmaxPredictor Maximum probability predictor. Source code in matfact/model/matfact.py 65 66 67 68 69 70 71 72 class ArgmaxPredictor : \"\"\"Maximum probability predictor.\"\"\" def fit ( self , matfact , observation_matrix ): ... def predict ( self , probabilities ): return np . argmax ( probabilities , axis = 1 ) + 1 ClassificationTreePredictor Threshold based predictor. Predict class from probabilities using thresholds biasing towards more rare states. See matfact.model.predict.classification_tree for details. Source code in matfact/model/matfact.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class ClassificationTreePredictor : \"\"\"Threshold based predictor. Predict class from probabilities using thresholds biasing towards more rare states. See [matfact.model.predict.classification_tree][] for details.\"\"\" _classification_tree : ClassificationTree def fit ( self , matfact , observation_matrix ): self . matfact = matfact self . _classification_tree = self . _estimate_classification_tree ( observation_matrix ) def predict ( self , probabilities ): return self . _classification_tree . predict ( probabilities ) def _estimate_classification_tree ( self , observation_matrix ): observation_matrix_masked , time_points , true_values = prediction_data ( observation_matrix ) probabilities = self . matfact . predict_probabilities ( observation_matrix_masked , time_points ) return estimate_probability_thresholds ( true_values , probabilities ) MatFact SKLearn like class for MatFact. Source code in matfact/model/matfact.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class MatFact : \"\"\"SKLearn like class for MatFact.\"\"\" _factorizer : BaseMF def __init__ ( self , config : ModelConfig , predictor : Predictor | None = None , probability_estimator : ProbabilityEstimator | None = None , model_factory = _model_factory , ): self . _predictor = predictor or ClassificationTreePredictor () self . _probability_estimator = ( probability_estimator or DefaultProbabilityEstimator () ) self . config = config self . _model_factory = model_factory def fit ( self , observation_matrix ): \"\"\"Fit the model.\"\"\" self . _factorizer = self . _model_factory ( observation_matrix , self . config ) self . _factorizer . matrix_completion () self . _predictor . fit ( self , observation_matrix ) return self def predict ( self , observation_matrix , time_points ): probabilities = self . predict_probabilities ( observation_matrix , time_points ) return self . _predictor . predict ( probabilities ) def predict_probabilities ( self , observation_matrix , time_points ): self . _check_is_fitted () return self . _probability_estimator . predict_probability ( self , observation_matrix , time_points ) def _check_is_fitted ( self ) -> None : if not hasattr ( self , \"_factorizer\" ): raise NotFittedException fit ( observation_matrix ) Fit the model. Source code in matfact/model/matfact.py 110 111 112 113 114 115 def fit ( self , observation_matrix ): \"\"\"Fit the model.\"\"\" self . _factorizer = self . _model_factory ( observation_matrix , self . config ) self . _factorizer . matrix_completion () self . _predictor . fit ( self , observation_matrix ) return self Predictor Bases: Protocol Predictor takes probabilities and gives a prediction. Source code in matfact/model/matfact.py 28 29 30 31 32 33 34 35 class Predictor ( Protocol ): \"\"\"Predictor takes probabilities and gives a prediction.\"\"\" def fit ( self , matfact , observation_matrix ) -> None : ... def predict ( self , probabilities ) -> npt . NDArray : ... ProbabilityEstimator Bases: Protocol Return probabilities for the different states. Source code in matfact/model/matfact.py 75 76 77 78 79 80 81 class ProbabilityEstimator ( Protocol ): \"\"\"Return probabilities for the different states.\"\"\" def predict_probability ( self , matfact , observation_matrix , time_points ) -> npt . NDArray : ... matfact.model.config ModelConfig dataclass Configuration class for the MatFact model. Source code in matfact/model/config.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 @dataclass ( frozen = True ) class ModelConfig : \"\"\"Configuration class for the MatFact model.\"\"\" shift_budget : list [ int ] = field ( default_factory = list ) lambda1 : float = 1.0 lambda2 : float = 1.0 lambda3 : float = 1.0 rank : int = 5 iter_U : int = 2 iter_V : int = 2 learning_rate : float = 0.001 number_of_states : int = settings . default_number_of_states difference_matrix_getter : Callable [[ int ], npt . NDArray ] = np . identity \"\"\"Return the difference matrix to use for regularization. Takes in the time dimension size of the observation matrix.\"\"\" weight_matrix_getter : WeightGetter = field ( default_factory = DataWeightGetter ) \"\"\"Return the weight matrix for a given observation matrix.\"\"\" minimal_value_matrix_getter : Callable [[ tuple [ int , int ]], npt . NDArray ] = np . ones \"\"\"Return the minium values for the V matrix. Takes in the shape of V. Warning: It is a known bug that the minimal value is not respected by all factorizers.\"\"\" initial_basic_profiles_getter : Callable [[ int , int ], npt . NDArray ] = initialize_basis \"\"\"Return the initial state for the basic profiles matrix V. Takes in the dimensions of the observation matrix.\"\"\" def get_short_model_name ( self ) -> str : \"\"\"Return a short string representing the model. The short name consists of three fields, shift, convolution, and weights. Sample names are scmf, l2mf.\"\"\" # Map possible difference_matrix_getters to string representations convolution_mapping = { np . identity : \"l2\" , convoluted_differences_matrix : \"c\" , } return ( \"\" . join ( ( \"s\" if self . shift_budget else \"\" , convolution_mapping . get ( self . difference_matrix_getter , \"?\" ), \"\" if self . weight_matrix_getter . is_identity else \"w\" , ) ) + \"mf\" ) difference_matrix_getter : Callable [[ int ], npt . NDArray ] = np . identity class-attribute Return the difference matrix to use for regularization. Takes in the time dimension size of the observation matrix. initial_basic_profiles_getter : Callable [[ int , int ], npt . NDArray ] = initialize_basis class-attribute Return the initial state for the basic profiles matrix V. Takes in the dimensions of the observation matrix. minimal_value_matrix_getter : Callable [[ tuple [ int , int ]], npt . NDArray ] = np . ones class-attribute Return the minium values for the V matrix. Takes in the shape of V. Warning It is a known bug that the minimal value is not respected by all factorizers. weight_matrix_getter : WeightGetter = field ( default_factory = DataWeightGetter ) class-attribute Return the weight matrix for a given observation matrix. get_short_model_name () Return a short string representing the model. The short name consists of three fields, shift, convolution, and weights. Sample names are scmf, l2mf. Source code in matfact/model/config.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def get_short_model_name ( self ) -> str : \"\"\"Return a short string representing the model. The short name consists of three fields, shift, convolution, and weights. Sample names are scmf, l2mf.\"\"\" # Map possible difference_matrix_getters to string representations convolution_mapping = { np . identity : \"l2\" , convoluted_differences_matrix : \"c\" , } return ( \"\" . join ( ( \"s\" if self . shift_budget else \"\" , convolution_mapping . get ( self . difference_matrix_getter , \"?\" ), \"\" if self . weight_matrix_getter . is_identity else \"w\" , ) ) + \"mf\" ) matfact.model train_and_log train_and_log ( X_train , X_test , * , epoch_generator = None , dict_to_log = None , extra_metrics = None , log_loss = True , logger_context = None , use_threshold_optimization = True , hyperparams ) Train model and log in MLFlow. Parameters: Name Type Description Default X_train np . ndarray Training data. required X_test np . ndarray Test data required dict_to_log Optional [ dict ] optional dictionary associated with the run, logged with MLFlow. None extra_metrics Optional [ dict [ str , Callable [[ Type [ BaseMF ]], float ]]] optional dictionary of metrics logged in each epoch of training. See BaseMF.matrix_completion for more details. None log_loss bool Whether the loss function as function of epoch should be logged in MLFlow. Note that this is slow. True nested If True, the run is logged as a nested run in MLFlow, useful in for example hyperparameter search. Assumes there to exist an active parent run. required use_threshold_optimization bool Use ClassificationTree optimization to find thresholds for class selection. Can improve results on data skewed towards normal. True Returns: Type Description A dictionary of relevant output statistics. Cross-validation Concerning cross validation: the function accepts a train and test set. In order to do for example cross validation hyperparameter search, simply wrap this function in cross validation logic. In this case, each run will be logged separately. In future versions of this package, it is possible that cross validation will be supported directly from within this function. However, it is not obvious what we should log, as we log for example the loss function of each training run. Two examples are to log each run separately or logging all folds together. Source code in matfact/model/util.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def train_and_log ( X_train : np . ndarray , X_test : np . ndarray , * , epoch_generator : EpochGenerator | None = None , dict_to_log : Optional [ dict ] = None , extra_metrics : Optional [ dict [ str , Callable [[ Type [ BaseMF ]], float ]]] = None , log_loss : bool = True , logger_context = None , use_threshold_optimization : bool = True , ** hyperparams , ): \"\"\"Train model and log in MLFlow. Arguments: X_train: Training data. X_test: Test data dict_to_log: optional dictionary associated with the run, logged with MLFlow. extra_metrics: optional dictionary of metrics logged in each epoch of training. See `BaseMF.matrix_completion` for more details. log_loss: Whether the loss function as function of epoch should be logged in MLFlow. Note that this is slow. nested: If True, the run is logged as a nested run in MLFlow, useful in for example hyperparameter search. Assumes there to exist an active parent run. use_threshold_optimization: Use ClassificationTree optimization to find thresholds for class selection. Can improve results on data skewed towards normal. Returns: A dictionary of relevant output statistics. !!! Note \"Cross-validation\" Concerning cross validation: the function accepts a train and test set. In order to do for example cross validation hyperparameter search, simply wrap this function in cross validation logic. In this case, each run will be logged separately. In future versions of this package, it is possible that cross validation will be supported directly from within this function. However, it is not obvious what we should log, as we log for example the loss function of each training run. Two examples are to log each run separately or logging all folds together. \"\"\" if logger_context is None : logger_context = MLFlowLogger () metrics = list ( extra_metrics . keys ()) if extra_metrics else [] if log_loss : if \"loss\" in metrics : raise ValueError ( \"log_loss True and loss is in extra_metrics. \" \"This is illegal, as it causes name collision!\" ) metrics . append ( \"loss\" ) with logger_context as logger : # Create model factoriser = model_factory ( X_train , ** hyperparams ) # Fit model results = factoriser . matrix_completion ( extra_metrics = extra_metrics , epoch_generator = epoch_generator ) # Predict X_test_masked , t_pred , x_true = prediction_data ( X_test ) p_pred = factoriser . predict_probability ( X_test_masked , t_pred ) mlflow_output : dict = { \"params\" : {}, \"metrics\" : {}, \"tags\" : {}, \"meta\" : {}, } if use_threshold_optimization : # Find the optimal threshold values X_train_masked , t_pred_train , x_true_train = prediction_data ( X_train ) p_pred_train = factoriser . predict_probability ( X_train_masked , t_pred_train ) classification_tree = estimate_probability_thresholds ( x_true_train , p_pred_train ) threshold_values = { f \"classification_tree_ { key } \" : value for key , value in classification_tree . get_params () . items () } mlflow_output [ \"params\" ] . update ( threshold_values ) # Use threshold values on the test set x_pred = classification_tree . predict ( p_pred ) else : # Simply choose the class with the highest probability # Class labels are 1-indexed, so add one to the arg index. x_pred = 1 + np . argmax ( p_pred , axis = 1 ) # Score score = matthews_corrcoef ( x_pred , x_true ) results . update ( { \"score\" : score , \"p_pred\" : p_pred , \"x_pred\" : x_pred , \"x_true\" : x_true , } ) mlflow_output [ \"meta\" ][ \"results\" ] = results # Logging mlflow_output [ \"params\" ] . update ( hyperparams ) mlflow_output [ \"params\" ][ \"model_name\" ] = factoriser . config . get_short_model_name () if dict_to_log : mlflow_output [ \"params\" ] . update ( dict_to_log ) mlflow_output [ \"metrics\" ][ \"matthew_score\" ] = score for metric in metrics : mlflow_output [ \"metrics\" ][ metric ] = results [ metric ] logger ( mlflow_output ) return mlflow_output matfact.model.factorization BaseMF Bases: ABC Base class for matrix factorization algorithms. Source code in matfact/model/factorization/factorizers/mfbase.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class BaseMF ( ABC ): \"Base class for matrix factorization algorithms.\" X : npt . NDArray U : npt . NDArray V : npt . NDArray @property @abstractmethod def M ( self ): ... @abstractmethod def run_step ( self ): return @abstractmethod def loss ( self ): return def predict_probability ( self , observed_data , t_pred ): \"\"\"Predict the probability of the possible states at t_pred\"\"\" return predict_proba ( observed_data , self . M , t_pred , theta_mle ( self . X , self . M ), number_of_states = self . config . number_of_states , ) def matrix_completion ( self , extra_metrics = None , epoch_generator : EpochGenerator | None = None , ): \"\"\"Run matrix completion on input matrix X using a factorization model. Arguments: extra_metrics: Dict of name, callable pairs for extra metric logging. Callable must have the signature `(model: Type[BaseMF]) -> Float`. epoch_generator: A generator of epoch numbers. Defaults to [`ConvergenceMonitor`][matfact.model.factorization.convergence.ConvergenceMonitor] which implements eager termination if detecting convergence. \"\"\" if epoch_generator is None : epoch_generator = ConvergenceMonitor () # Results collected from the process output : dict = { \"loss\" : [], \"epochs\" : [], \"U\" : None , \"V\" : None , \"M\" : None , \"s\" : None , \"theta_mle\" : None , } if extra_metrics is None : extra_metrics = {} for metric in extra_metrics : output [ metric ] = [] for epoch in epoch_generator ( self ): self . run_step () output [ \"epochs\" ] . append ( int ( epoch )) output [ \"loss\" ] . append ( float ( self . loss ())) for metric , callable in extra_metrics . items (): output [ metric ] . append ( callable ( self )) output [ \"U\" ] = self . U output [ \"V\" ] = self . V output [ \"M\" ] = self . M output [ \"s\" ] = getattr ( self , \"s\" , None ) output [ \"theta_mle\" ] = theta_mle ( self . X , self . M ) return output matrix_completion ( extra_metrics = None , epoch_generator = None ) Run matrix completion on input matrix X using a factorization model. Parameters: Name Type Description Default extra_metrics Dict of name, callable pairs for extra metric logging. Callable must have the signature (model: Type[BaseMF]) -> Float . None epoch_generator EpochGenerator | None A generator of epoch numbers. Defaults to ConvergenceMonitor which implements eager termination if detecting convergence. None Source code in matfact/model/factorization/factorizers/mfbase.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def matrix_completion ( self , extra_metrics = None , epoch_generator : EpochGenerator | None = None , ): \"\"\"Run matrix completion on input matrix X using a factorization model. Arguments: extra_metrics: Dict of name, callable pairs for extra metric logging. Callable must have the signature `(model: Type[BaseMF]) -> Float`. epoch_generator: A generator of epoch numbers. Defaults to [`ConvergenceMonitor`][matfact.model.factorization.convergence.ConvergenceMonitor] which implements eager termination if detecting convergence. \"\"\" if epoch_generator is None : epoch_generator = ConvergenceMonitor () # Results collected from the process output : dict = { \"loss\" : [], \"epochs\" : [], \"U\" : None , \"V\" : None , \"M\" : None , \"s\" : None , \"theta_mle\" : None , } if extra_metrics is None : extra_metrics = {} for metric in extra_metrics : output [ metric ] = [] for epoch in epoch_generator ( self ): self . run_step () output [ \"epochs\" ] . append ( int ( epoch )) output [ \"loss\" ] . append ( float ( self . loss ())) for metric , callable in extra_metrics . items (): output [ metric ] . append ( callable ( self )) output [ \"U\" ] = self . U output [ \"V\" ] = self . V output [ \"M\" ] = self . M output [ \"s\" ] = getattr ( self , \"s\" , None ) output [ \"theta_mle\" ] = theta_mle ( self . X , self . M ) return output predict_probability ( observed_data , t_pred ) Predict the probability of the possible states at t_pred Source code in matfact/model/factorization/factorizers/mfbase.py 30 31 32 33 34 35 36 37 38 def predict_probability ( self , observed_data , t_pred ): \"\"\"Predict the probability of the possible states at t_pred\"\"\" return predict_proba ( observed_data , self . M , t_pred , theta_mle ( self . X , self . M ), number_of_states = self . config . number_of_states , ) CMF Bases: BaseMF Matrix factorization with L2 and convolutional regularization. Factor updates are based on analytical estimates and will therefore not permit and arbitrary weight matrix in the discrepancy term. Parameters: Name Type Description Default X Sparse data matrix used to estimate factor matrices required V Initial estimate for basic vectors required config ModelConfig Configuration model. shift and weights are ignored in the CMF factorizer. required Source code in matfact/model/factorization/factorizers/cmf.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 class CMF ( BaseMF ): \"\"\"Matrix factorization with L2 and convolutional regularization. Factor updates are based on analytical estimates and will therefore not permit and arbitrary weight matrix in the discrepancy term. Args: X: Sparse data matrix used to estimate factor matrices V: Initial estimate for basic vectors config: Configuration model. shift and weights are ignored in the CMF factorizer. \"\"\" def __init__ ( self , X , config : ModelConfig , ): if not config . weight_matrix_getter . is_identity : warn ( \"CMF given a non-identity weight. This will be ignored.\" \"Consider using WCMF or SCMF.\" ) if config . shift_budget : warn ( \"CMF given a non-empty shift budget. This will be ignored.\" \"Consider using SCMF.\" ) self . X = X self . V = config . initial_basic_profiles_getter ( X . shape [ 1 ], config . rank ) self . config = config self . N , self . T = np . shape ( self . X ) self . nz_rows , self . nz_cols = np . nonzero ( self . X ) self . n_iter_ = 0 KD = self . config . difference_matrix_getter ( self . T ) self . J = self . config . minimal_value_matrix_getter (( self . T , self . config . rank )) self . _init_matrices ( KD ) self . _update_U () @property def M ( self ): return np . array ( self . U @ self . V . T , dtype = np . float32 ) def _init_matrices ( self , KD ): self . S = self . X . copy () self . mask = ( self . X != 0 ) . astype ( np . float32 ) self . I_l1 = self . config . lambda1 * np . identity ( self . config . rank ) self . I_l2 = self . config . lambda2 * np . identity ( self . config . rank ) self . KD = KD self . DTKTKD = KD . T @ KD self . L2 , self . Q2 = np . linalg . eigh ( self . config . lambda3 * self . DTKTKD ) def _update_V ( self ): L1 , Q1 = np . linalg . eigh ( self . U . T @ self . U + self . I_l2 ) hatV = ( ( self . Q2 . T @ ( self . S . T @ self . U + self . config . lambda2 * self . J )) @ Q1 / np . add . outer ( self . L2 , L1 ) ) self . V = self . Q2 @ ( hatV @ Q1 . T ) def _update_U ( self ): self . U = np . transpose ( np . linalg . solve ( self . V . T @ self . V + self . I_l1 , self . V . T @ self . S . T ) ) def _update_S ( self ): self . S = self . U @ self . V . T self . S [ self . nz_rows , self . nz_cols ] = self . X [ self . nz_rows , self . nz_cols ] def loss ( self ): \"Compute the loss from the optimization objective\" loss = np . square ( np . linalg . norm ( self . mask * ( self . X - self . U @ self . V . T ))) loss += self . config . lambda1 * np . square ( np . linalg . norm ( self . U )) loss += self . config . lambda2 * np . square ( np . linalg . norm ( self . V - self . J )) loss += self . config . lambda3 * np . square ( np . linalg . norm ( self . KD @ self . V )) return loss def run_step ( self ): \"Perform one step of alternating minimization\" self . _update_U () self . _update_V () self . _update_S () self . n_iter_ += 1 SCMF Bases: BaseMF Shifted matrix factorization with L2 and convolutional regularization (optional). Factor updates are based on gradient descent approximations, permitting an arbitrary weight matrix in the discrepancy term. The shift mechanism will maximize the correlation between vector samples in the original and estimated data matrices for more accurate factor estimates. Parameters: Name Type Description Default X Sparse data matrix used to estimate factor matrices required V Initial estimate for basic vectors required config ModelConfig Configuration model. required Discussion on internal matrices There are four X matrices (correspondingly for W): X : The original input matrix X_bc : The original input matrix with padded zeros on the time axis. X_shifted : Similar to X_bc, but each row is shifted according to self.s. X_shifts : A stack of size(s_budged) arrays similar to X_bc, but each shifted horizontally (time axis). Stack layer i is shifted s_budged[i]. X_shifted is the only matrix altered after initialization. Source code in matfact/model/factorization/factorizers/scmf.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 class SCMF ( BaseMF ): \"\"\"Shifted matrix factorization with L2 and convolutional regularization (optional). Factor updates are based on gradient descent approximations, permitting an arbitrary weight matrix in the discrepancy term. The shift mechanism will maximize the correlation between vector samples in the original and estimated data matrices for more accurate factor estimates. Args: X: Sparse data matrix used to estimate factor matrices V: Initial estimate for basic vectors config: Configuration model. Discussion on internal matrices: There are four X matrices (correspondingly for W): - X : The original input matrix - X_bc : The original input matrix with padded zeros on the time axis. - X_shifted : Similar to X_bc, but each row is shifted according to self.s. - X_shifts : A stack of size(s_budged) arrays similar to X_bc, but each shifted horizontally (time axis). Stack layer i is shifted s_budged[i]. X_shifted is the only matrix altered after initialization. \"\"\" def __init__ ( self , X , config : ModelConfig , ): self . config = config self . W = self . config . weight_matrix_getter ( X ) V = config . initial_basic_profiles_getter ( X . shape [ 1 ], config . rank ) self . N , self . T = np . shape ( X ) self . nz_rows , self . nz_cols = np . nonzero ( X ) self . n_iter_ = 0 # The shift amount per row self . s = np . zeros ( self . N , dtype = int ) # The number of possible shifts. Used for padding of arrays. self . Ns = len ( self . config . shift_budget ) # Add time points to cover extended left and right boundaries when shifting. self . KD = tf . cast ( self . config . difference_matrix_getter ( self . T + 2 * self . Ns ), dtype = tf . float32 ) self . I1 = self . config . lambda1 * np . identity ( self . config . rank ) self . I2 = self . config . lambda2 * np . identity ( self . config . rank ) # Expand matrices with zeros over the extended left and right boundaries. self . X_bc = np . hstack ( [ np . zeros (( self . N , self . Ns )), X , np . zeros (( self . N , self . Ns ))] ) self . W_bc = np . hstack ( [ np . zeros (( self . N , self . Ns )), self . W , np . zeros (( self . N , self . Ns ))] ) self . V_bc = np . vstack ( [ np . zeros (( self . Ns , self . config . rank )), V , np . zeros (( self . Ns , self . config . rank )), ] ) # We know V_bc to be two-dimensional, so cast to please mypy. J_shape = cast ( tuple [ int , int ], self . V_bc . shape ) # TODO: do we have to cast this to tf float32? self . J = self . config . minimal_value_matrix_getter ( J_shape ) # Implementation shifts W and Y (not UV.T) self . X_shifted = self . X_bc . copy () self . W_shifted = self . W_bc . copy () self . _fill_boundary_regions_V_bc () # Placeholders (s x N x T) for all possible candidate shits self . X_shifts = np . empty (( self . Ns , * self . X_bc . shape )) self . W_shifts = np . empty (( self . Ns , * self . W_bc . shape )) # Shift Y in opposite direction of V shift. for j , s_n in enumerate ( self . config . shift_budget ): self . X_shifts [ j ] = np . roll ( self . X_bc , - 1 * s_n , axis = 1 ) self . W_shifts [ j ] = np . roll ( self . W_bc , - 1 * s_n , axis = 1 ) self . U = self . _exactly_solve_U () @property def X ( self ): # return self.X_bc[:, self.Ns:-self.Ns] return _take_per_row_strided ( self . X_shifted , self . Ns - self . s , n_elem = self . T ) @property def V ( self ): \"\"\"To be compatible with the expectation of having a V\"\"\" return self . V_bc @property def M ( self ): # Compute the reconstructed matrix with sample-specific shifts M = _take_per_row_strided ( self . U @ self . V_bc . T , start_idx = self . Ns - self . s , n_elem = self . T ) return np . array ( M , dtype = np . float32 ) def _shift_X_W ( self ): self . X_shifted = _custom_roll ( self . X_bc , - 1 * self . s ) self . W_shifted = _custom_roll ( self . W_bc , - 1 * self . s ) def _fill_boundary_regions_V_bc ( self ): \"\"\"Extrapolate the edge values in V_bc over the extended boundaries\"\"\" V_filled = np . zeros_like ( self . V_bc ) idx = np . arange ( self . T + 2 * self . Ns ) for i , v in enumerate ( self . V_bc . T ): v_left = v [ idx <= int ( self . T / 2 )] v_right = v [ idx > int ( self . T / 2 )] v_left [ v_left == 0 ] = v_left [ np . argmax ( v_left != 0 )] v_right [ v_right == 0 ] = v_right [ np . argmax ( np . cumsum ( v_right != 0 ))] V_filled [:, i ] = np . concatenate ([ v_left , v_right ]) self . V_bc = V_filled def _update_V ( self ): V = tf . Variable ( self . V_bc , dtype = tf . float32 ) # @tf.function def _loss_V (): frob_tensor = tf . multiply ( self . W_shifted , self . X_shifted - ( self . U @ tf . transpose ( V )) ) frob_loss = tf . reduce_sum ( frob_tensor ** 2 ) l2_loss = self . config . lambda2 * tf . reduce_sum (( V - self . J ) ** 2 ) conv_loss = self . config . lambda3 * tf . reduce_sum ( ( tf . matmul ( self . KD , V ) ** 2 ) ) return frob_loss + l2_loss + conv_loss optimiser = tf . keras . optimizers . Adam ( learning_rate = self . config . learning_rate ) for _ in tf . range ( self . config . iter_V ): optimiser . minimize ( _loss_V , [ V ]) self . V_bc = V . numpy () def _exactly_solve_U ( self ): \"\"\"Solve for U at a fixed V. The internal U member is not modified by this method. V is assumed to be initialized.\"\"\" U = np . empty (( self . N , self . config . rank )) for n in range ( self . N ): U [ n ] = ( self . X_shifted [ n ] @ self . V_bc @ np . linalg . inv ( self . V_bc . T @ ( np . diag ( self . W_shifted [ n ]) @ self . V_bc ) + self . I1 ) ) return U def _approx_U ( self ): U = tf . Variable ( self . U , dtype = tf . float32 ) # @tf.function def _loss_U (): frob_tensor = tf . multiply ( self . W_shifted , self . X_shifted - tf . matmul ( U , self . V_bc , transpose_b = True ), ) frob_loss = tf . reduce_sum (( frob_tensor ) ** 2 ) return frob_loss + self . config . lambda1 * tf . reduce_sum ( U ** 2 ) optimiser = tf . keras . optimizers . Adam ( learning_rate = self . config . learning_rate ) for _ in tf . range ( self . config . iter_U ): optimiser . minimize ( _loss_U , [ U ]) return U . numpy () def _update_s ( self ): # Evaluate the discrepancy term for all possible shift candidates M = self . U @ self . V_bc . T D = ( np . linalg . norm ( self . W_shifts * ( self . X_shifts - M [ None , :, :]), axis =- 1 ) ** 2 ) # Selected shifts maximize the correlation between X and M s_new = [ self . config . shift_budget [ i ] for i in np . argmin ( D , axis = 0 )] # Update attributes only if changes to the optimal shift if not np . array_equal ( self . s , s_new ): self . s = np . array ( s_new ) self . _shift_X_W () def run_step ( self ): \"Perform one step of alternating minimization\" self . U = self . _approx_U () self . _update_V () self . _update_s () self . n_iter_ += 1 def loss ( self ): \"Compute the loss from the optimization objective\" loss = np . sum ( np . linalg . norm ( self . W_shifted * ( self . X_shifted - self . U @ self . V_bc . T ), axis = 1 ) ** 2 ) loss += self . config . lambda1 * np . sum ( np . linalg . norm ( self . U , axis = 1 ) ** 2 ) loss += self . config . lambda2 * np . linalg . norm ( self . V_bc ) ** 2 loss += self . config . lambda3 * np . linalg . norm ( self . KD @ self . V_bc ) ** 2 return loss V property To be compatible with the expectation of having a V WCMF Bases: BaseMF Matrix factorization with L2 and convolutional regularization. Factor updates are based on gradient descent approximations, permitting an arbitrary weight matrix in the discrepancy term. Parameters: Name Type Description Default X Sparse data matrix used to estimate factor matrices required V Initial estimate for basic vectors required config ModelConfig Configuration model. shift is ignored in the WCMF factorizer. required Source code in matfact/model/factorization/factorizers/wcmf.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 class WCMF ( BaseMF ): \"\"\"Matrix factorization with L2 and convolutional regularization. Factor updates are based on gradient descent approximations, permitting an arbitrary weight matrix in the discrepancy term. Args: X: Sparse data matrix used to estimate factor matrices V: Initial estimate for basic vectors config: Configuration model. shift is ignored in the WCMF factorizer. \"\"\" def __init__ ( self , X , config : ModelConfig , ): if config . shift_budget : warn ( \"WCMF given a non-empty shift budget. This will be ignored.\" \"Consider using SCMF.\" ) self . config = config self . X = X self . V = config . initial_basic_profiles_getter ( X . shape [ 1 ], config . rank ) self . W = self . config . weight_matrix_getter ( X ) self . N , self . T = np . shape ( self . X ) self . nz_rows , self . nz_cols = np . nonzero ( self . X ) self . n_iter_ = 0 KD = self . config . difference_matrix_getter ( self . T ) self . J = self . config . minimal_value_matrix_getter (( self . T , self . config . rank )) self . _init_matrices ( KD ) self . U = self . _exactly_solve_U () @property def M ( self ): return np . array ( self . U @ self . V . T , dtype = np . float32 ) def _init_matrices ( self , KD ): self . KD = tf . cast ( KD , dtype = tf . float32 ) self . DTKTKD = KD . T @ KD self . I_l1 = self . config . lambda1 * np . eye ( self . config . rank ) def _update_V ( self ): # @tf.function def _loss_V (): frob_tensor = tf . multiply ( W , X - ( U @ tf . transpose ( V ))) frob_loss = tf . square ( tf . norm ( frob_tensor )) l2_loss = self . config . lambda2 * tf . square ( tf . norm ( V - J )) conv_loss = self . config . lambda3 * tf . square ( tf . norm ( tf . matmul ( self . KD , V ))) return frob_loss + l2_loss + conv_loss V = tf . Variable ( self . V , dtype = tf . float32 ) J = tf . ones_like ( self . V , dtype = tf . float32 ) W = tf . cast ( self . W , dtype = tf . float32 ) X = tf . cast ( self . X , dtype = tf . float32 ) U = tf . cast ( self . U , dtype = tf . float32 ) optimiser = tf . keras . optimizers . Adam ( learning_rate = self . config . learning_rate ) for _ in tf . range ( self . config . iter_V ): optimiser . minimize ( _loss_V , [ V ]) self . V = V . numpy () def _exactly_solve_U ( self ): \"\"\"Solve for U at a fixed V. The internal U member is not modified by this method. V is assumed to be initialized.\"\"\" U = np . empty (( self . N , self . config . rank )) for n in range ( self . N ): U [ n ] = ( self . V . T @ ( self . W [ n ] * self . X [ n ]) @ np . linalg . inv ( self . V . T @ ( self . W [ n ][:, None ] * self . V ) + self . I_l1 ) ) return U def _approx_U ( self ): # @tf.function def _loss_U (): frob_tensor = tf . multiply ( W , X - tf . matmul ( U , V , transpose_b = True )) frob_loss = tf . square ( tf . norm ( frob_tensor )) return frob_loss + self . config . lambda1 * tf . square ( tf . norm ( U )) U = tf . Variable ( self . U , dtype = tf . float32 ) W = tf . cast ( self . W , dtype = tf . float32 ) X = tf . cast ( self . X , dtype = tf . float32 ) V = tf . cast ( self . V , dtype = tf . float32 ) optimiser = tf . keras . optimizers . Adam ( learning_rate = self . config . learning_rate ) for _ in tf . range ( self . config . iter_U ): optimiser . minimize ( _loss_U , [ U ]) return U . numpy () def loss ( self ): \"Compute the loss from the optimization objective\" loss = np . square ( np . linalg . norm ( self . W * ( self . X - self . U @ self . V . T ))) loss += self . config . lambda1 * np . square ( np . linalg . norm ( self . U )) loss += self . config . lambda2 * np . square ( np . linalg . norm ( self . V - 1 )) loss += self . config . lambda3 * np . square ( np . linalg . norm ( self . KD @ self . V )) return loss def run_step ( self ): \"Perform one step of alternating minimization\" self . U = self . _approx_U () self . _update_V () self . n_iter_ += 1 ConvergenceMonitor Epoch generator with eager termination when converged. The model is said to have converged when the model's latent matrix (M) has a relative norm difference smaller than tolerance. Parameters: Name Type Description Default number_of_epochs int the maximum number of epochs to generate. DEFAULT_NUMBER_OF_EPOCHS epochs_per_val int convergence checking is done every epochs_per_val epoch. DEFAULT_EPOCHS_PER_VAL patience int the minimum number of epcohs. DEFAULT_PATIENCE show_progress bool enable tqdm progress bar. True tolerance float the tolerance under which the model is said to have converged. 0.0001 Examples: monitor = ConvergenceMonitor ( tolerance = 1e-5 ) for epoch in monitor ( model ): # If model converges, the generator will deplete before the default number # of epochs has been reached. ... Source code in matfact/model/factorization/convergence.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class ConvergenceMonitor : \"\"\"Epoch generator with eager termination when converged. The model is said to have converged when the model's latent matrix (M) has a relative norm difference smaller than tolerance. Args: number_of_epochs: the maximum number of epochs to generate. epochs_per_val: convergence checking is done every epochs_per_val epoch. patience: the minimum number of epcohs. show_progress: enable tqdm progress bar. tolerance: the tolerance under which the model is said to have converged. Examples: ```python monitor = ConvergenceMonitor(tolerance=1e-5) for epoch in monitor(model): # If model converges, the generator will deplete before the default number # of epochs has been reached. ... ``` \"\"\" def __init__ ( self , number_of_epochs : int = DEFAULT_NUMBER_OF_EPOCHS , epochs_per_val : int = DEFAULT_EPOCHS_PER_VAL , patience : int = DEFAULT_PATIENCE , show_progress : bool = True , tolerance : float = 1e-4 , ): self . number_of_epochs = number_of_epochs self . tolerance = tolerance self . epochs_per_val = epochs_per_val self . patience = patience self . _range = trange if show_progress else range @staticmethod def _difference_func ( new_M , old_M ): return np . sum (( new_M - old_M ) ** 2 ) / np . sum ( old_M ** 2 ) def __call__ ( self , model : \"BaseMF\" ): \"\"\"A generator that yields epoch numbers.\"\"\" _old_M = model . M _model = model # mypy does not recognize the union of trange and range as a callable. for i in self . _range ( self . number_of_epochs ): # type: ignore yield i # model is expected to update its M should_update = i > self . patience and i % self . epochs_per_val == 0 if should_update : if self . _difference_func ( _model . M , _old_M ) < self . tolerance : break _old_M = model . M __call__ ( model ) A generator that yields epoch numbers. Source code in matfact/model/factorization/convergence.py 64 65 66 67 68 69 70 71 72 73 74 75 def __call__ ( self , model : \"BaseMF\" ): \"\"\"A generator that yields epoch numbers.\"\"\" _old_M = model . M _model = model # mypy does not recognize the union of trange and range as a callable. for i in self . _range ( self . number_of_epochs ): # type: ignore yield i # model is expected to update its M should_update = i > self . patience and i % self . epochs_per_val == 0 if should_update : if self . _difference_func ( _model . M , _old_M ) < self . tolerance : break _old_M = model . M matfact.model.logging Logging utilities. The logging is written in a general way, however at the moment only MLFlow is supported as backend. MLFlowBatchLogger Bases: MLFlowLogger Context manager for combining multiple run data into one MLFlow run. Given several run dictionaries, the data is aggregated together to one summary run, which is logged to MLFlow. Used in for example cross validation runs, where each fold is a subrun, and the entire cross validation is logged as one run. Examples: It is possible to run MLFlowBatchLogger wrapped around subrun contexts. >>> with MLFlowBatchLogger () as outer_logger : >>> for subrun in subruns : >>> with MLFlowLogger () as inner_logger : >>> ... >>> inner_logger ( run_data ) >>> outer_logger ( run_data ) # (1) The data is only logged to MLFlow when MLFlowBatchLogger exists. How and when are things logged? Each call to the logger is stored as a subrun, which on exit is aggregated together to one run which is logged to MLFLow. See matfact.model.logging.batch_mlflow_logger for details. Source code in matfact/model/logging.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 class MLFlowBatchLogger ( MLFlowLogger ): \"\"\"Context manager for combining multiple run data into one MLFlow run. Given several run dictionaries, the data is aggregated together to one summary run, which is logged to MLFlow. Used in for example cross validation runs, where each fold is a subrun, and the entire cross validation is logged as one run. Examples: It is possible to run MLFlowBatchLogger wrapped around subrun contexts. >>> with MLFlowBatchLogger() as outer_logger: >>> for subrun in subruns: >>> with MLFlowLogger() as inner_logger: >>> ... >>> inner_logger(run_data) >>> outer_logger(run_data) # (1) 1. The data is only logged to MLFlow when `MLFlowBatchLogger` exists. ??? Note \"How and when are things logged?\" Each call to the logger is stored as a subrun, which on exit is aggregated together to one run which is logged to MLFLow. See [matfact.model.logging.batch_mlflow_logger][] for details. \"\"\" def __init__ ( self , allow_nesting : bool = True , extra_tags : dict | None = None , aggregate_funcs : list [ AggregationFunction ] | None = None , ) -> None : super () . __init__ ( allow_nesting = allow_nesting , extra_tags = extra_tags ) self . aggregate_funcs = aggregate_funcs def __enter__ ( self ): self . output = [] return super () . __enter__ () def __exit__ ( self , type , value , traceback ): batch_mlflow_logger ( self . output , aggregate_funcs = self . aggregate_funcs ) return super () . __exit__ ( type , value , traceback ) def __call__ ( self , output_dict ): # On call we only append the dict to our list of data. # The actual logging happens on __exit__. self . output . append ( output_dict ) if self . extra_tags : mlflow . set_tags ( self . extra_tags ) MLFlowLogger Context manager for MLFlow logging. Wraps the code inside the corresponding with block in an MLFlow run. Parameters: Name Type Description Default allow_nesting bool loggers can be nested within each other, with inside runs being logged as children in MLFlow. True extra_tags dict | None these tags will be appended to each run. None Examples: with MLFlowLogger () as logger : output = get_output_data () # output = { # \"params\": {...}, # \"metrics\": {...}, # \"meta\": {...}, # } logger ( output ) Raises: Type Description MLFlowRunHierarchyException Raised on enter if loggers are nested when allow_nesting is False. See also MLFlowBatchLogger MLFlowLoggerArtifact MLFlowLoggerDiagnostic Source code in matfact/model/logging.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 class MLFlowLogger : \"\"\"Context manager for MLFlow logging. Wraps the code inside the corresponding with block in an MLFlow run. Arguments: allow_nesting: loggers can be nested within each other, with inside runs being logged as children in MLFlow. extra_tags: these tags will be appended to each run. Examples: ```python with MLFlowLogger() as logger: output = get_output_data() # output = { # \"params\": {...}, # \"metrics\": {...}, # \"meta\": {...}, # } logger(output) ``` Raises: MLFlowRunHierarchyException: Raised on enter if loggers are nested when allow_nesting is False. See also: - [MLFlowBatchLogger][matfact.model.logging.MLFlowBatchLogger] - [MLFlowLoggerArtifact][matfact.model.logging.MLFlowLoggerArtifact] - [MLFlowLoggerDiagnostic][matfact.model.logging.MLFlowLoggerDiagnostic] \"\"\" def __init__ ( self , allow_nesting : bool = True , extra_tags : dict | None = None ): self . allow_nesting = allow_nesting self . extra_tags = extra_tags if extra_tags else {} def __enter__ ( self ): try : self . run_ = mlflow . start_run ( nested = self . allow_nesting ) except Exception as e : if re . match ( \"Run with UUID [0-9a-f]+ is already active.\" , str ( e )): self . __exit__ ( type ( e ), str ( e ), e . __traceback__ ) raise MLFlowRunHierarchyException ( \"allow_nesting is False, but loggers are nested!\" ) return self def __exit__ ( self , type , value , traceback ): \"\"\"End the run by calling the underlying ActiveRun's exit method.\"\"\" if hasattr ( self , \"run_\" ): return self . run_ . __exit__ ( type , value , traceback ) else : return True def __call__ ( self , output_dict : dict ): \"\"\"Log an output dict to MLFlow.\"\"\" mlflow_logger ( output_dict ) mlflow . set_tags ( self . extra_tags ) MLFlowLoggerArtifact Bases: MLFlowLogger Context manager for MLFlow logging, with artifact logging. All artifacts in artifact_path will be logged to the MLFlow run. Source code in matfact/model/logging.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 class MLFlowLoggerArtifact ( MLFlowLogger ): \"\"\"Context manager for MLFlow logging, with artifact logging. All artifacts in artifact_path will be logged to the MLFlow run.\"\"\" def __init__ ( self , artifact_path : pathlib . Path , allow_nesting : bool = True , extra_tags : dict | None = None , create_artifact_path : bool = settings . create_path_default , ): super () . __init__ ( allow_nesting = allow_nesting , extra_tags = extra_tags ) self . figure_path = artifact_path if create_artifact_path : artifact_path . mkdir ( parents = True , exist_ok = True ) def __call__ ( self , output_dict ): super () . __call__ ( output_dict ) mlflow . log_artifacts ( self . figure_path ) MLFlowLoggerDiagnostic Bases: MLFlowLoggerArtifact Context manager for MLFlow logging, generating default diagnostic plots. Source code in matfact/model/logging.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 class MLFlowLoggerDiagnostic ( MLFlowLoggerArtifact ): \"\"\"Context manager for MLFlow logging, generating default diagnostic plots.\"\"\" def __call__ ( self , output_dict ): solver_output = output_dict [ \"meta\" ][ \"results\" ] plot_coefs ( solver_output [ \"U\" ], self . figure_path ) plot_basis ( solver_output [ \"V\" ], self . figure_path ) number_of_states = solver_output [ \"p_pred\" ] . shape [ 1 ] plot_confusion ( solver_output [ \"x_true\" ], solver_output [ \"x_pred\" ], self . figure_path , n_classes = number_of_states , ) plot_roc_curve ( solver_output [ \"x_true\" ], solver_output [ \"p_pred\" ], self . figure_path , number_of_states = number_of_states , ) plot_certainty ( solver_output [ \"p_pred\" ], solver_output [ \"x_true\" ], self . figure_path ) super () . __call__ ( output_dict ) batch_mlflow_logger ( log_data , aggregate_funcs = None ) Combine and log a set of runs. Used in for example cross validation training, where all folds should be logged as one run. Parameters: Name Type Description Default log_data list [ dict ] list of run data. Each entry in log_data should be compatible with the format expected by _mlflow_logger . { \"params\" : { \"field1\" : value1 ,}, \"metrics\" : { \"field2\" : foo , \"field_history\" : [ ... ],}, \"tags\" : {}, } required Source code in matfact/model/logging.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def batch_mlflow_logger ( log_data : list [ dict ], aggregate_funcs : list [ AggregationFunction ] | None = None ) -> None : \"\"\"Combine and log a set of runs. Used in for example cross validation training, where all folds should be logged as one run. Arguments: log_data: list of run data. Each entry in log_data should be compatible with the format expected by `_mlflow_logger`. ```python { \"params\": {\"field1\": value1,}, \"metrics\": {\"field2\": foo, \"field_history\": [...],}, \"tags\": {}, } ``` \"\"\" new_log : dict = { \"params\" : {}, \"metrics\" : {}, \"tags\" : {}, } new_log [ \"params\" ] = _aggregate_fields ( [ data [ \"params\" ] for data in log_data ], aggregate_funcs = aggregate_funcs ) new_log [ \"metrics\" ] = _aggregate_fields ( [ data [ \"metrics\" ] for data in log_data ], aggregate_funcs = aggregate_funcs ) mlflow_logger ( new_log ) mlflow_logger ( log_data ) Log results dictionary to MLFlow. Given a dictionary on the format below, add the run to mlflow. Assumes there to be an active MLFlow run! The run is typically started by MLFLowLogger . Params and metrics should have values that are floats. Metric also accepts a list of floats, in which case they are interpreted as the metric value as a function of the epochs. { \"params\" : { \"param1\" : value , \"param2\" : value ,}, \"metrics\" : { \"metric1\" : value , \"metric2\" : [ ... ]}, \"tags\" : {}, \"meta\" : {}, # Data not logged to MLFLow } Source code in matfact/model/logging.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def mlflow_logger ( log_data : dict ) -> None : \"\"\"Log results dictionary to MLFlow. Given a dictionary on the format below, add the run to mlflow. Assumes there to be an active MLFlow run! The run is typically started by [`MLFLowLogger`][matfact.model.logging.MLFlowLogger]. Params and metrics should have values that are floats. Metric also accepts a list of floats, in which case they are interpreted as the metric value as a function of the epochs. ```python { \"params\": {\"param1\": value, \"param2\": value,}, \"metrics\": {\"metric1\": value, \"metric2\": [...]}, \"tags\": {}, \"meta\": {}, # Data not logged to MLFLow } ``` \"\"\" for parameter , value in log_data [ \"params\" ] . items (): mlflow . log_param ( parameter , value ) for metric , value in log_data [ \"metrics\" ] . items (): if isinstance ( value , list ): for epoch , value_at_epoch in enumerate ( value ): mlflow . log_metric ( metric , value_at_epoch , step = epoch ) else : mlflow . log_metric ( metric , value ) mlflow . set_tags ( log_data [ \"tags\" ]) matfact.data_generation.Dataset Screening dataset container This class simplifies generating, loading, and saving datasets. Chaining Most methods returns the Dataset object, so that it is chainable, as Dataset . from_file ( some_path ) . get_X_M () Source code in matfact/data_generation/dataset.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 class Dataset : \"\"\"Screening dataset container This class simplifies generating, loading, and saving datasets. !!! Note \"Chaining\" Most methods returns the Dataset object, so that it is chainable, as ```python Dataset.from_file(some_path).get_X_M() ``` \"\"\" def __init__ ( self , X : np . ndarray , M : np . ndarray , metadata : dict ): self . X = X self . M = M self . metadata = metadata def __str__ ( self ): return ( \"Dataset object of size\" f \" { self . metadata [ 'N' ] } x { self . metadata [ 'T' ] } with rank\" f \" { self . metadata [ 'rank' ] } and sparsity level\" f \" { self . metadata [ 'sparsity_level' ] } \" ) @classmethod def from_file ( cls , path : pathlib . Path ): \"\"\"Load dataset from file\"\"\" X , M = np . load ( path / \"X.npy\" ), np . load ( path / \"M.npy\" ) with ( path / \"dataset_metadata.json\" ) . open ( \"r\" ) as metadata_file : metadata = json . load ( metadata_file ) return cls ( X , M , metadata ) def save ( self , path : pathlib . Path ): \"\"\"Store dataset to file\"\"\" np . save ( path / \"X.npy\" , self . X ) np . save ( path / \"M.npy\" , self . M ) with ( path / \"dataset_metadata.json\" ) . open ( \"w\" ) as metadata_file : metadata_file . write ( json . dumps ( self . metadata )) return self @classmethod def generate ( cls , N , T , rank , sparsity_level , produce_dataset_function = produce_dataset , number_of_states = default_number_of_states , observation_probabilities = default_observation_probabilities , ): \"\"\"Generate a Dataset produce_dataset_function should be a callable with signature ``` Callable( N, T, rank, sparsity_level, *, number_of_states, observation_probabilities ) -> observed_matrix: ndarray, latent_matrix: ndarray, generation_name: str ``` \"\"\" X , M , generation_name = produce_dataset_function ( N , T , rank , sparsity_level , number_of_states = number_of_states , observation_probabilities = observation_probabilities , ) number_of_individuals = X . shape [ 0 ] if number_of_individuals == 0 : raise RuntimeError ( \"Data generation produced no valid screening data!\" ) metadata = { \"rank\" : rank , \"sparsity_level\" : sparsity_level , \"N\" : N , \"T\" : T , \"generation_method\" : generation_name , \"number_of_states\" : number_of_states , \"observation_probabilities\" : list ( observation_probabilities ), } return cls ( X , M , metadata ) def get_X_M ( self ): \"\"\"Return the X and M matrix.\"\"\" return self . X , self . M def get_split_X_M ( self , ratio = 0.8 ): \"\"\"Split dataset into train and test subsets.\"\"\" X , M = self . get_X_M () slice_index = int ( X . shape [ 0 ] * ratio ) return X [: slice_index ], X [ slice_index :], M [: slice_index ], M [ slice_index :] def prefixed_metadata ( self , prefix = \"DATASET_\" ): \"\"\"Return the metadata dict with prefix prepended to keys Convenience method used in for example logging\"\"\" return { prefix + key : value for key , value in self . metadata . items ()} from_file ( path ) classmethod Load dataset from file Source code in matfact/data_generation/dataset.py 116 117 118 119 120 121 122 @classmethod def from_file ( cls , path : pathlib . Path ): \"\"\"Load dataset from file\"\"\" X , M = np . load ( path / \"X.npy\" ), np . load ( path / \"M.npy\" ) with ( path / \"dataset_metadata.json\" ) . open ( \"r\" ) as metadata_file : metadata = json . load ( metadata_file ) return cls ( X , M , metadata ) generate ( N , T , rank , sparsity_level , produce_dataset_function = produce_dataset , number_of_states = default_number_of_states , observation_probabilities = default_observation_probabilities ) classmethod Generate a Dataset produce_dataset_function should be a callable with signature Callable( N, T, rank, sparsity_level, *, number_of_states, observation_probabilities ) -> observed_matrix: ndarray, latent_matrix: ndarray, generation_name: str Source code in matfact/data_generation/dataset.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 @classmethod def generate ( cls , N , T , rank , sparsity_level , produce_dataset_function = produce_dataset , number_of_states = default_number_of_states , observation_probabilities = default_observation_probabilities , ): \"\"\"Generate a Dataset produce_dataset_function should be a callable with signature ``` Callable( N, T, rank, sparsity_level, *, number_of_states, observation_probabilities ) -> observed_matrix: ndarray, latent_matrix: ndarray, generation_name: str ``` \"\"\" X , M , generation_name = produce_dataset_function ( N , T , rank , sparsity_level , number_of_states = number_of_states , observation_probabilities = observation_probabilities , ) number_of_individuals = X . shape [ 0 ] if number_of_individuals == 0 : raise RuntimeError ( \"Data generation produced no valid screening data!\" ) metadata = { \"rank\" : rank , \"sparsity_level\" : sparsity_level , \"N\" : N , \"T\" : T , \"generation_method\" : generation_name , \"number_of_states\" : number_of_states , \"observation_probabilities\" : list ( observation_probabilities ), } return cls ( X , M , metadata ) get_X_M () Return the X and M matrix. Source code in matfact/data_generation/dataset.py 176 177 178 def get_X_M ( self ): \"\"\"Return the X and M matrix.\"\"\" return self . X , self . M get_split_X_M ( ratio = 0.8 ) Split dataset into train and test subsets. Source code in matfact/data_generation/dataset.py 180 181 182 183 184 def get_split_X_M ( self , ratio = 0.8 ): \"\"\"Split dataset into train and test subsets.\"\"\" X , M = self . get_X_M () slice_index = int ( X . shape [ 0 ] * ratio ) return X [: slice_index ], X [ slice_index :], M [: slice_index ], M [ slice_index :] prefixed_metadata ( prefix = 'DATASET_' ) Return the metadata dict with prefix prepended to keys Convenience method used in for example logging Source code in matfact/data_generation/dataset.py 186 187 188 189 190 def prefixed_metadata ( self , prefix = \"DATASET_\" ): \"\"\"Return the metadata dict with prefix prepended to keys Convenience method used in for example logging\"\"\" return { prefix + key : value for key , value in self . metadata . items ()} save ( path ) Store dataset to file Source code in matfact/data_generation/dataset.py 124 125 126 127 128 129 130 def save ( self , path : pathlib . Path ): \"\"\"Store dataset to file\"\"\" np . save ( path / \"X.npy\" , self . X ) np . save ( path / \"M.npy\" , self . M ) with ( path / \"dataset_metadata.json\" ) . open ( \"w\" ) as metadata_file : metadata_file . write ( json . dumps ( self . metadata )) return self Utility functions for masking prediction data used in training. prediction_data ( observation_matrix , prediction_time_strategy = _last_observed_time , row_masking_strategy = _mask_row ) Mask observed data to produce test data for predictions. Parameters: Name Type Description Default prediction_time_strategy Callable [[ npt . NDArray ], npt . NDArray ] Callable returning prediction times given the observation_matrix. Defaults to choosing the last observation. _last_observed_time row_masking_strategy Callable [[ npt . NDArray , int ], None] Callable masking an observation row, i.e. individual history, given a time point. Note that the masking is done in place, not by return. Defaults to masking the three time slots from prediction time and back. _mask_row Examples: >>> masked_observation , times , correct_values = prediction_data ( my_data ) >>> my_data [ range ( times . size ), times ] == correct_values True >>> ( masked_observation [ range ( times . size ), times ] == 0 ) . all () True Source code in matfact/model/predict/dataset_utils.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def prediction_data ( observation_matrix : npt . NDArray , prediction_time_strategy : Callable [ [ npt . NDArray ], npt . NDArray ] = _last_observed_time , row_masking_strategy : Callable [[ npt . NDArray , int ], None ] = _mask_row , ) -> tuple [ npt . NDArray , npt . NDArray , npt . NDArray ]: \"\"\"Mask observed data to produce test data for predictions. Arguments: prediction_time_strategy: Callable returning prediction times given the observation_matrix. Defaults to choosing the last observation. row_masking_strategy: Callable masking an observation row, i.e. individual history, given a time point. Note that the masking is done in place, not by return. Defaults to masking the three time slots from prediction time and back. Examples: >>> masked_observation, times, correct_values = prediction_data(my_data) >>> my_data[range(times.size), times] == correct_values True >>> (masked_observation[range(times.size), times] == 0).all() True \"\"\" times = prediction_time_strategy ( observation_matrix ) # Choose the value at times for each row correct_values = np . take_along_axis ( observation_matrix , times [:, None ], axis = 1 ) . flatten () # row_masking_strategy masks in-place, so important to copy! masked_observation_matrix = np . copy ( observation_matrix ) for i , row in enumerate ( masked_observation_matrix ): row_masking_strategy ( row , times [ i ]) return masked_observation_matrix , times , correct_values Threshold based classification. To correct for the data being skewed, we introduce some threshold values that favor less likely classes, even when they do not have the highest probability. Consider we have some probabilities [p0, p1, p2], where pi is the probability of class i. We introduce thresholds [t1, t2], such that instead of choosing the class with the highest probability, we choose the class as follows: beginning from the class with the highest number (assumed to be the rearest), check if p2 > t2. If so, we set the class to p2. Next, check p1 > t1, etc. In general, given probabilities [p0, p1, p2, ...] and thresholds [t1, t2, ...], set the class to max(i) where pi > ti. This prediction algorithm is implemented in ClassificationTree . In addition, estimate_probability_thresholds estimates the optimal values of the thresholds. ClassificationTree Bases: BaseEstimator , ClassifierMixin Perform hierarchical classification given probability thresholds. Class labels are 1 indexed integers. The number of thresholds (tau) is one less than the number of classes. Source code in matfact/model/predict/classification_tree.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class ClassificationTree ( BaseEstimator , ClassifierMixin ): \"\"\"Perform hierarchical classification given probability thresholds. Class labels are 1 indexed integers. The number of thresholds (tau) is one less than the number of classes. \"\"\" def __init__ ( self , thresholds : Sequence [ float ] | None = None ): self . thresholds = thresholds if thresholds is not None else [] def predict ( self , probabilities : np . ndarray ): \"\"\"Perform classification given probabilities for classes. Arguments: probabilities: (number_of_samples x number_of_classes) ndarray \"\"\" number_of_samples , number_of_classes = probabilities . shape if number_of_classes != len ( self . thresholds ) + 1 : raise ValueError ( f \"Probabilities for { number_of_classes } classes given. \" \"The number of thresholds should be one less than the number of classes\" f \", but it is { len ( self . thresholds ) } .\" ) # Set all samples to class one. # Iterate through the classes, if the probability is above the # threshold, assign that class. classes = np . ones ( number_of_samples ) for i , threshold in enumerate ( self . thresholds ): # Threshold i correspond to class i + 1, so add one classes [ probabilities [:, i + 1 ] >= threshold ] = i + 1 return classes def fit ( self , X : Any , y : Any ): \"\"\"Do nothing, fit required by sklearn API specification.\"\"\" return self fit ( X , y ) Do nothing, fit required by sklearn API specification. Source code in matfact/model/predict/classification_tree.py 62 63 64 def fit ( self , X : Any , y : Any ): \"\"\"Do nothing, fit required by sklearn API specification.\"\"\" return self predict ( probabilities ) Perform classification given probabilities for classes. probabilities: (number_of_samples x number_of_classes) ndarray Source code in matfact/model/predict/classification_tree.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def predict ( self , probabilities : np . ndarray ): \"\"\"Perform classification given probabilities for classes. Arguments: probabilities: (number_of_samples x number_of_classes) ndarray \"\"\" number_of_samples , number_of_classes = probabilities . shape if number_of_classes != len ( self . thresholds ) + 1 : raise ValueError ( f \"Probabilities for { number_of_classes } classes given. \" \"The number of thresholds should be one less than the number of classes\" f \", but it is { len ( self . thresholds ) } .\" ) # Set all samples to class one. # Iterate through the classes, if the probability is above the # threshold, assign that class. classes = np . ones ( number_of_samples ) for i , threshold in enumerate ( self . thresholds ): # Threshold i correspond to class i + 1, so add one classes [ probabilities [:, i + 1 ] >= threshold ] = i + 1 return classes estimate_probability_thresholds ( y_true , y_predicted_probabilities , tol = 1e-06 , seed = 42 ) Estimate threshold values for ClassificationTree with differential evolution. Parameters: Name Type Description Default y_true np . ndarray Vector of class labels. required y_pred_proba Vector of predicted probabilities. required Returns: Type Description A ClassificationTree object instantiated with the estimated probability thresholds. This object may be saved to disk using scikit-learn routines. Source code in matfact/model/predict/classification_tree.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def estimate_probability_thresholds ( y_true : np . ndarray , y_predicted_probabilities : np . ndarray , tol : float = 1e-6 , seed : int = 42 , ): \"\"\"Estimate threshold values for ClassificationTree with differential evolution. Args: y_true: Vector of class labels. y_pred_proba: Vector of predicted probabilities. Returns: A ClassificationTree object instantiated with the estimated probability thresholds. This object may be saved to disk using scikit-learn routines. \"\"\" check_X_y ( y_predicted_probabilities , y_true ) number_of_classes = y_predicted_probabilities . shape [ 1 ] result = optimize . differential_evolution ( _matthews_correlation_coefficient_objective , # Bounds are [0, 1] for each threshold value, i.e. one less than the number # of classes. Iterators are not accepted, so convert to list. bounds = list ( itertools . repeat (( 0 , 1 ), number_of_classes - 1 )), args = ( y_true , y_predicted_probabilities , ClassificationTree ()), seed = seed , tol = tol , ) return ClassificationTree ( thresholds = result . x )","title":"MatFact"},{"location":"matfact/#matfact","text":"Matrix Factorization with temporal regularization. Example from matfact.model import train_and_log from matfact.data_generation import Dataset data = Dataset . generate () X_train , X_test , _ , _ = data . get_split_X_M () train_and_log ( X_train , X_test )","title":"MatFact"},{"location":"matfact/#matfact.model.matfact","text":"","title":"matfact"},{"location":"matfact/#matfact.model.matfact.ArgmaxPredictor","text":"Maximum probability predictor. Source code in matfact/model/matfact.py 65 66 67 68 69 70 71 72 class ArgmaxPredictor : \"\"\"Maximum probability predictor.\"\"\" def fit ( self , matfact , observation_matrix ): ... def predict ( self , probabilities ): return np . argmax ( probabilities , axis = 1 ) + 1","title":"ArgmaxPredictor"},{"location":"matfact/#matfact.model.matfact.ClassificationTreePredictor","text":"Threshold based predictor. Predict class from probabilities using thresholds biasing towards more rare states. See matfact.model.predict.classification_tree for details. Source code in matfact/model/matfact.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class ClassificationTreePredictor : \"\"\"Threshold based predictor. Predict class from probabilities using thresholds biasing towards more rare states. See [matfact.model.predict.classification_tree][] for details.\"\"\" _classification_tree : ClassificationTree def fit ( self , matfact , observation_matrix ): self . matfact = matfact self . _classification_tree = self . _estimate_classification_tree ( observation_matrix ) def predict ( self , probabilities ): return self . _classification_tree . predict ( probabilities ) def _estimate_classification_tree ( self , observation_matrix ): observation_matrix_masked , time_points , true_values = prediction_data ( observation_matrix ) probabilities = self . matfact . predict_probabilities ( observation_matrix_masked , time_points ) return estimate_probability_thresholds ( true_values , probabilities )","title":"ClassificationTreePredictor"},{"location":"matfact/#matfact.model.matfact.MatFact","text":"SKLearn like class for MatFact. Source code in matfact/model/matfact.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class MatFact : \"\"\"SKLearn like class for MatFact.\"\"\" _factorizer : BaseMF def __init__ ( self , config : ModelConfig , predictor : Predictor | None = None , probability_estimator : ProbabilityEstimator | None = None , model_factory = _model_factory , ): self . _predictor = predictor or ClassificationTreePredictor () self . _probability_estimator = ( probability_estimator or DefaultProbabilityEstimator () ) self . config = config self . _model_factory = model_factory def fit ( self , observation_matrix ): \"\"\"Fit the model.\"\"\" self . _factorizer = self . _model_factory ( observation_matrix , self . config ) self . _factorizer . matrix_completion () self . _predictor . fit ( self , observation_matrix ) return self def predict ( self , observation_matrix , time_points ): probabilities = self . predict_probabilities ( observation_matrix , time_points ) return self . _predictor . predict ( probabilities ) def predict_probabilities ( self , observation_matrix , time_points ): self . _check_is_fitted () return self . _probability_estimator . predict_probability ( self , observation_matrix , time_points ) def _check_is_fitted ( self ) -> None : if not hasattr ( self , \"_factorizer\" ): raise NotFittedException","title":"MatFact"},{"location":"matfact/#matfact.model.matfact.MatFact.fit","text":"Fit the model. Source code in matfact/model/matfact.py 110 111 112 113 114 115 def fit ( self , observation_matrix ): \"\"\"Fit the model.\"\"\" self . _factorizer = self . _model_factory ( observation_matrix , self . config ) self . _factorizer . matrix_completion () self . _predictor . fit ( self , observation_matrix ) return self","title":"fit()"},{"location":"matfact/#matfact.model.matfact.Predictor","text":"Bases: Protocol Predictor takes probabilities and gives a prediction. Source code in matfact/model/matfact.py 28 29 30 31 32 33 34 35 class Predictor ( Protocol ): \"\"\"Predictor takes probabilities and gives a prediction.\"\"\" def fit ( self , matfact , observation_matrix ) -> None : ... def predict ( self , probabilities ) -> npt . NDArray : ...","title":"Predictor"},{"location":"matfact/#matfact.model.matfact.ProbabilityEstimator","text":"Bases: Protocol Return probabilities for the different states. Source code in matfact/model/matfact.py 75 76 77 78 79 80 81 class ProbabilityEstimator ( Protocol ): \"\"\"Return probabilities for the different states.\"\"\" def predict_probability ( self , matfact , observation_matrix , time_points ) -> npt . NDArray : ...","title":"ProbabilityEstimator"},{"location":"matfact/#matfact.model.config","text":"","title":"config"},{"location":"matfact/#matfact.model.config.ModelConfig","text":"Configuration class for the MatFact model. Source code in matfact/model/config.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 @dataclass ( frozen = True ) class ModelConfig : \"\"\"Configuration class for the MatFact model.\"\"\" shift_budget : list [ int ] = field ( default_factory = list ) lambda1 : float = 1.0 lambda2 : float = 1.0 lambda3 : float = 1.0 rank : int = 5 iter_U : int = 2 iter_V : int = 2 learning_rate : float = 0.001 number_of_states : int = settings . default_number_of_states difference_matrix_getter : Callable [[ int ], npt . NDArray ] = np . identity \"\"\"Return the difference matrix to use for regularization. Takes in the time dimension size of the observation matrix.\"\"\" weight_matrix_getter : WeightGetter = field ( default_factory = DataWeightGetter ) \"\"\"Return the weight matrix for a given observation matrix.\"\"\" minimal_value_matrix_getter : Callable [[ tuple [ int , int ]], npt . NDArray ] = np . ones \"\"\"Return the minium values for the V matrix. Takes in the shape of V. Warning: It is a known bug that the minimal value is not respected by all factorizers.\"\"\" initial_basic_profiles_getter : Callable [[ int , int ], npt . NDArray ] = initialize_basis \"\"\"Return the initial state for the basic profiles matrix V. Takes in the dimensions of the observation matrix.\"\"\" def get_short_model_name ( self ) -> str : \"\"\"Return a short string representing the model. The short name consists of three fields, shift, convolution, and weights. Sample names are scmf, l2mf.\"\"\" # Map possible difference_matrix_getters to string representations convolution_mapping = { np . identity : \"l2\" , convoluted_differences_matrix : \"c\" , } return ( \"\" . join ( ( \"s\" if self . shift_budget else \"\" , convolution_mapping . get ( self . difference_matrix_getter , \"?\" ), \"\" if self . weight_matrix_getter . is_identity else \"w\" , ) ) + \"mf\" )","title":"ModelConfig"},{"location":"matfact/#matfact.model.config.ModelConfig.difference_matrix_getter","text":"Return the difference matrix to use for regularization. Takes in the time dimension size of the observation matrix.","title":"difference_matrix_getter"},{"location":"matfact/#matfact.model.config.ModelConfig.initial_basic_profiles_getter","text":"Return the initial state for the basic profiles matrix V. Takes in the dimensions of the observation matrix.","title":"initial_basic_profiles_getter"},{"location":"matfact/#matfact.model.config.ModelConfig.minimal_value_matrix_getter","text":"Return the minium values for the V matrix. Takes in the shape of V. Warning It is a known bug that the minimal value is not respected by all factorizers.","title":"minimal_value_matrix_getter"},{"location":"matfact/#matfact.model.config.ModelConfig.weight_matrix_getter","text":"Return the weight matrix for a given observation matrix.","title":"weight_matrix_getter"},{"location":"matfact/#matfact.model.config.ModelConfig.get_short_model_name","text":"Return a short string representing the model. The short name consists of three fields, shift, convolution, and weights. Sample names are scmf, l2mf. Source code in matfact/model/config.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def get_short_model_name ( self ) -> str : \"\"\"Return a short string representing the model. The short name consists of three fields, shift, convolution, and weights. Sample names are scmf, l2mf.\"\"\" # Map possible difference_matrix_getters to string representations convolution_mapping = { np . identity : \"l2\" , convoluted_differences_matrix : \"c\" , } return ( \"\" . join ( ( \"s\" if self . shift_budget else \"\" , convolution_mapping . get ( self . difference_matrix_getter , \"?\" ), \"\" if self . weight_matrix_getter . is_identity else \"w\" , ) ) + \"mf\" )","title":"get_short_model_name()"},{"location":"matfact/#matfact.model","text":"","title":"model"},{"location":"matfact/#matfact.model.train_and_log","text":"train_and_log ( X_train , X_test , * , epoch_generator = None , dict_to_log = None , extra_metrics = None , log_loss = True , logger_context = None , use_threshold_optimization = True , hyperparams ) Train model and log in MLFlow. Parameters: Name Type Description Default X_train np . ndarray Training data. required X_test np . ndarray Test data required dict_to_log Optional [ dict ] optional dictionary associated with the run, logged with MLFlow. None extra_metrics Optional [ dict [ str , Callable [[ Type [ BaseMF ]], float ]]] optional dictionary of metrics logged in each epoch of training. See BaseMF.matrix_completion for more details. None log_loss bool Whether the loss function as function of epoch should be logged in MLFlow. Note that this is slow. True nested If True, the run is logged as a nested run in MLFlow, useful in for example hyperparameter search. Assumes there to exist an active parent run. required use_threshold_optimization bool Use ClassificationTree optimization to find thresholds for class selection. Can improve results on data skewed towards normal. True Returns: Type Description A dictionary of relevant output statistics. Cross-validation Concerning cross validation: the function accepts a train and test set. In order to do for example cross validation hyperparameter search, simply wrap this function in cross validation logic. In this case, each run will be logged separately. In future versions of this package, it is possible that cross validation will be supported directly from within this function. However, it is not obvious what we should log, as we log for example the loss function of each training run. Two examples are to log each run separately or logging all folds together. Source code in matfact/model/util.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def train_and_log ( X_train : np . ndarray , X_test : np . ndarray , * , epoch_generator : EpochGenerator | None = None , dict_to_log : Optional [ dict ] = None , extra_metrics : Optional [ dict [ str , Callable [[ Type [ BaseMF ]], float ]]] = None , log_loss : bool = True , logger_context = None , use_threshold_optimization : bool = True , ** hyperparams , ): \"\"\"Train model and log in MLFlow. Arguments: X_train: Training data. X_test: Test data dict_to_log: optional dictionary associated with the run, logged with MLFlow. extra_metrics: optional dictionary of metrics logged in each epoch of training. See `BaseMF.matrix_completion` for more details. log_loss: Whether the loss function as function of epoch should be logged in MLFlow. Note that this is slow. nested: If True, the run is logged as a nested run in MLFlow, useful in for example hyperparameter search. Assumes there to exist an active parent run. use_threshold_optimization: Use ClassificationTree optimization to find thresholds for class selection. Can improve results on data skewed towards normal. Returns: A dictionary of relevant output statistics. !!! Note \"Cross-validation\" Concerning cross validation: the function accepts a train and test set. In order to do for example cross validation hyperparameter search, simply wrap this function in cross validation logic. In this case, each run will be logged separately. In future versions of this package, it is possible that cross validation will be supported directly from within this function. However, it is not obvious what we should log, as we log for example the loss function of each training run. Two examples are to log each run separately or logging all folds together. \"\"\" if logger_context is None : logger_context = MLFlowLogger () metrics = list ( extra_metrics . keys ()) if extra_metrics else [] if log_loss : if \"loss\" in metrics : raise ValueError ( \"log_loss True and loss is in extra_metrics. \" \"This is illegal, as it causes name collision!\" ) metrics . append ( \"loss\" ) with logger_context as logger : # Create model factoriser = model_factory ( X_train , ** hyperparams ) # Fit model results = factoriser . matrix_completion ( extra_metrics = extra_metrics , epoch_generator = epoch_generator ) # Predict X_test_masked , t_pred , x_true = prediction_data ( X_test ) p_pred = factoriser . predict_probability ( X_test_masked , t_pred ) mlflow_output : dict = { \"params\" : {}, \"metrics\" : {}, \"tags\" : {}, \"meta\" : {}, } if use_threshold_optimization : # Find the optimal threshold values X_train_masked , t_pred_train , x_true_train = prediction_data ( X_train ) p_pred_train = factoriser . predict_probability ( X_train_masked , t_pred_train ) classification_tree = estimate_probability_thresholds ( x_true_train , p_pred_train ) threshold_values = { f \"classification_tree_ { key } \" : value for key , value in classification_tree . get_params () . items () } mlflow_output [ \"params\" ] . update ( threshold_values ) # Use threshold values on the test set x_pred = classification_tree . predict ( p_pred ) else : # Simply choose the class with the highest probability # Class labels are 1-indexed, so add one to the arg index. x_pred = 1 + np . argmax ( p_pred , axis = 1 ) # Score score = matthews_corrcoef ( x_pred , x_true ) results . update ( { \"score\" : score , \"p_pred\" : p_pred , \"x_pred\" : x_pred , \"x_true\" : x_true , } ) mlflow_output [ \"meta\" ][ \"results\" ] = results # Logging mlflow_output [ \"params\" ] . update ( hyperparams ) mlflow_output [ \"params\" ][ \"model_name\" ] = factoriser . config . get_short_model_name () if dict_to_log : mlflow_output [ \"params\" ] . update ( dict_to_log ) mlflow_output [ \"metrics\" ][ \"matthew_score\" ] = score for metric in metrics : mlflow_output [ \"metrics\" ][ metric ] = results [ metric ] logger ( mlflow_output ) return mlflow_output","title":"train_and_log()"},{"location":"matfact/#matfact.model.factorization","text":"","title":"factorization"},{"location":"matfact/#matfact.model.factorization.BaseMF","text":"Bases: ABC Base class for matrix factorization algorithms. Source code in matfact/model/factorization/factorizers/mfbase.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class BaseMF ( ABC ): \"Base class for matrix factorization algorithms.\" X : npt . NDArray U : npt . NDArray V : npt . NDArray @property @abstractmethod def M ( self ): ... @abstractmethod def run_step ( self ): return @abstractmethod def loss ( self ): return def predict_probability ( self , observed_data , t_pred ): \"\"\"Predict the probability of the possible states at t_pred\"\"\" return predict_proba ( observed_data , self . M , t_pred , theta_mle ( self . X , self . M ), number_of_states = self . config . number_of_states , ) def matrix_completion ( self , extra_metrics = None , epoch_generator : EpochGenerator | None = None , ): \"\"\"Run matrix completion on input matrix X using a factorization model. Arguments: extra_metrics: Dict of name, callable pairs for extra metric logging. Callable must have the signature `(model: Type[BaseMF]) -> Float`. epoch_generator: A generator of epoch numbers. Defaults to [`ConvergenceMonitor`][matfact.model.factorization.convergence.ConvergenceMonitor] which implements eager termination if detecting convergence. \"\"\" if epoch_generator is None : epoch_generator = ConvergenceMonitor () # Results collected from the process output : dict = { \"loss\" : [], \"epochs\" : [], \"U\" : None , \"V\" : None , \"M\" : None , \"s\" : None , \"theta_mle\" : None , } if extra_metrics is None : extra_metrics = {} for metric in extra_metrics : output [ metric ] = [] for epoch in epoch_generator ( self ): self . run_step () output [ \"epochs\" ] . append ( int ( epoch )) output [ \"loss\" ] . append ( float ( self . loss ())) for metric , callable in extra_metrics . items (): output [ metric ] . append ( callable ( self )) output [ \"U\" ] = self . U output [ \"V\" ] = self . V output [ \"M\" ] = self . M output [ \"s\" ] = getattr ( self , \"s\" , None ) output [ \"theta_mle\" ] = theta_mle ( self . X , self . M ) return output","title":"BaseMF"},{"location":"matfact/#matfact.model.factorization.factorizers.mfbase.BaseMF.matrix_completion","text":"Run matrix completion on input matrix X using a factorization model. Parameters: Name Type Description Default extra_metrics Dict of name, callable pairs for extra metric logging. Callable must have the signature (model: Type[BaseMF]) -> Float . None epoch_generator EpochGenerator | None A generator of epoch numbers. Defaults to ConvergenceMonitor which implements eager termination if detecting convergence. None Source code in matfact/model/factorization/factorizers/mfbase.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def matrix_completion ( self , extra_metrics = None , epoch_generator : EpochGenerator | None = None , ): \"\"\"Run matrix completion on input matrix X using a factorization model. Arguments: extra_metrics: Dict of name, callable pairs for extra metric logging. Callable must have the signature `(model: Type[BaseMF]) -> Float`. epoch_generator: A generator of epoch numbers. Defaults to [`ConvergenceMonitor`][matfact.model.factorization.convergence.ConvergenceMonitor] which implements eager termination if detecting convergence. \"\"\" if epoch_generator is None : epoch_generator = ConvergenceMonitor () # Results collected from the process output : dict = { \"loss\" : [], \"epochs\" : [], \"U\" : None , \"V\" : None , \"M\" : None , \"s\" : None , \"theta_mle\" : None , } if extra_metrics is None : extra_metrics = {} for metric in extra_metrics : output [ metric ] = [] for epoch in epoch_generator ( self ): self . run_step () output [ \"epochs\" ] . append ( int ( epoch )) output [ \"loss\" ] . append ( float ( self . loss ())) for metric , callable in extra_metrics . items (): output [ metric ] . append ( callable ( self )) output [ \"U\" ] = self . U output [ \"V\" ] = self . V output [ \"M\" ] = self . M output [ \"s\" ] = getattr ( self , \"s\" , None ) output [ \"theta_mle\" ] = theta_mle ( self . X , self . M ) return output","title":"matrix_completion()"},{"location":"matfact/#matfact.model.factorization.factorizers.mfbase.BaseMF.predict_probability","text":"Predict the probability of the possible states at t_pred Source code in matfact/model/factorization/factorizers/mfbase.py 30 31 32 33 34 35 36 37 38 def predict_probability ( self , observed_data , t_pred ): \"\"\"Predict the probability of the possible states at t_pred\"\"\" return predict_proba ( observed_data , self . M , t_pred , theta_mle ( self . X , self . M ), number_of_states = self . config . number_of_states , )","title":"predict_probability()"},{"location":"matfact/#matfact.model.factorization.CMF","text":"Bases: BaseMF Matrix factorization with L2 and convolutional regularization. Factor updates are based on analytical estimates and will therefore not permit and arbitrary weight matrix in the discrepancy term. Parameters: Name Type Description Default X Sparse data matrix used to estimate factor matrices required V Initial estimate for basic vectors required config ModelConfig Configuration model. shift and weights are ignored in the CMF factorizer. required Source code in matfact/model/factorization/factorizers/cmf.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 class CMF ( BaseMF ): \"\"\"Matrix factorization with L2 and convolutional regularization. Factor updates are based on analytical estimates and will therefore not permit and arbitrary weight matrix in the discrepancy term. Args: X: Sparse data matrix used to estimate factor matrices V: Initial estimate for basic vectors config: Configuration model. shift and weights are ignored in the CMF factorizer. \"\"\" def __init__ ( self , X , config : ModelConfig , ): if not config . weight_matrix_getter . is_identity : warn ( \"CMF given a non-identity weight. This will be ignored.\" \"Consider using WCMF or SCMF.\" ) if config . shift_budget : warn ( \"CMF given a non-empty shift budget. This will be ignored.\" \"Consider using SCMF.\" ) self . X = X self . V = config . initial_basic_profiles_getter ( X . shape [ 1 ], config . rank ) self . config = config self . N , self . T = np . shape ( self . X ) self . nz_rows , self . nz_cols = np . nonzero ( self . X ) self . n_iter_ = 0 KD = self . config . difference_matrix_getter ( self . T ) self . J = self . config . minimal_value_matrix_getter (( self . T , self . config . rank )) self . _init_matrices ( KD ) self . _update_U () @property def M ( self ): return np . array ( self . U @ self . V . T , dtype = np . float32 ) def _init_matrices ( self , KD ): self . S = self . X . copy () self . mask = ( self . X != 0 ) . astype ( np . float32 ) self . I_l1 = self . config . lambda1 * np . identity ( self . config . rank ) self . I_l2 = self . config . lambda2 * np . identity ( self . config . rank ) self . KD = KD self . DTKTKD = KD . T @ KD self . L2 , self . Q2 = np . linalg . eigh ( self . config . lambda3 * self . DTKTKD ) def _update_V ( self ): L1 , Q1 = np . linalg . eigh ( self . U . T @ self . U + self . I_l2 ) hatV = ( ( self . Q2 . T @ ( self . S . T @ self . U + self . config . lambda2 * self . J )) @ Q1 / np . add . outer ( self . L2 , L1 ) ) self . V = self . Q2 @ ( hatV @ Q1 . T ) def _update_U ( self ): self . U = np . transpose ( np . linalg . solve ( self . V . T @ self . V + self . I_l1 , self . V . T @ self . S . T ) ) def _update_S ( self ): self . S = self . U @ self . V . T self . S [ self . nz_rows , self . nz_cols ] = self . X [ self . nz_rows , self . nz_cols ] def loss ( self ): \"Compute the loss from the optimization objective\" loss = np . square ( np . linalg . norm ( self . mask * ( self . X - self . U @ self . V . T ))) loss += self . config . lambda1 * np . square ( np . linalg . norm ( self . U )) loss += self . config . lambda2 * np . square ( np . linalg . norm ( self . V - self . J )) loss += self . config . lambda3 * np . square ( np . linalg . norm ( self . KD @ self . V )) return loss def run_step ( self ): \"Perform one step of alternating minimization\" self . _update_U () self . _update_V () self . _update_S () self . n_iter_ += 1","title":"CMF"},{"location":"matfact/#matfact.model.factorization.SCMF","text":"Bases: BaseMF Shifted matrix factorization with L2 and convolutional regularization (optional). Factor updates are based on gradient descent approximations, permitting an arbitrary weight matrix in the discrepancy term. The shift mechanism will maximize the correlation between vector samples in the original and estimated data matrices for more accurate factor estimates. Parameters: Name Type Description Default X Sparse data matrix used to estimate factor matrices required V Initial estimate for basic vectors required config ModelConfig Configuration model. required Discussion on internal matrices There are four X matrices (correspondingly for W): X : The original input matrix X_bc : The original input matrix with padded zeros on the time axis. X_shifted : Similar to X_bc, but each row is shifted according to self.s. X_shifts : A stack of size(s_budged) arrays similar to X_bc, but each shifted horizontally (time axis). Stack layer i is shifted s_budged[i]. X_shifted is the only matrix altered after initialization. Source code in matfact/model/factorization/factorizers/scmf.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 class SCMF ( BaseMF ): \"\"\"Shifted matrix factorization with L2 and convolutional regularization (optional). Factor updates are based on gradient descent approximations, permitting an arbitrary weight matrix in the discrepancy term. The shift mechanism will maximize the correlation between vector samples in the original and estimated data matrices for more accurate factor estimates. Args: X: Sparse data matrix used to estimate factor matrices V: Initial estimate for basic vectors config: Configuration model. Discussion on internal matrices: There are four X matrices (correspondingly for W): - X : The original input matrix - X_bc : The original input matrix with padded zeros on the time axis. - X_shifted : Similar to X_bc, but each row is shifted according to self.s. - X_shifts : A stack of size(s_budged) arrays similar to X_bc, but each shifted horizontally (time axis). Stack layer i is shifted s_budged[i]. X_shifted is the only matrix altered after initialization. \"\"\" def __init__ ( self , X , config : ModelConfig , ): self . config = config self . W = self . config . weight_matrix_getter ( X ) V = config . initial_basic_profiles_getter ( X . shape [ 1 ], config . rank ) self . N , self . T = np . shape ( X ) self . nz_rows , self . nz_cols = np . nonzero ( X ) self . n_iter_ = 0 # The shift amount per row self . s = np . zeros ( self . N , dtype = int ) # The number of possible shifts. Used for padding of arrays. self . Ns = len ( self . config . shift_budget ) # Add time points to cover extended left and right boundaries when shifting. self . KD = tf . cast ( self . config . difference_matrix_getter ( self . T + 2 * self . Ns ), dtype = tf . float32 ) self . I1 = self . config . lambda1 * np . identity ( self . config . rank ) self . I2 = self . config . lambda2 * np . identity ( self . config . rank ) # Expand matrices with zeros over the extended left and right boundaries. self . X_bc = np . hstack ( [ np . zeros (( self . N , self . Ns )), X , np . zeros (( self . N , self . Ns ))] ) self . W_bc = np . hstack ( [ np . zeros (( self . N , self . Ns )), self . W , np . zeros (( self . N , self . Ns ))] ) self . V_bc = np . vstack ( [ np . zeros (( self . Ns , self . config . rank )), V , np . zeros (( self . Ns , self . config . rank )), ] ) # We know V_bc to be two-dimensional, so cast to please mypy. J_shape = cast ( tuple [ int , int ], self . V_bc . shape ) # TODO: do we have to cast this to tf float32? self . J = self . config . minimal_value_matrix_getter ( J_shape ) # Implementation shifts W and Y (not UV.T) self . X_shifted = self . X_bc . copy () self . W_shifted = self . W_bc . copy () self . _fill_boundary_regions_V_bc () # Placeholders (s x N x T) for all possible candidate shits self . X_shifts = np . empty (( self . Ns , * self . X_bc . shape )) self . W_shifts = np . empty (( self . Ns , * self . W_bc . shape )) # Shift Y in opposite direction of V shift. for j , s_n in enumerate ( self . config . shift_budget ): self . X_shifts [ j ] = np . roll ( self . X_bc , - 1 * s_n , axis = 1 ) self . W_shifts [ j ] = np . roll ( self . W_bc , - 1 * s_n , axis = 1 ) self . U = self . _exactly_solve_U () @property def X ( self ): # return self.X_bc[:, self.Ns:-self.Ns] return _take_per_row_strided ( self . X_shifted , self . Ns - self . s , n_elem = self . T ) @property def V ( self ): \"\"\"To be compatible with the expectation of having a V\"\"\" return self . V_bc @property def M ( self ): # Compute the reconstructed matrix with sample-specific shifts M = _take_per_row_strided ( self . U @ self . V_bc . T , start_idx = self . Ns - self . s , n_elem = self . T ) return np . array ( M , dtype = np . float32 ) def _shift_X_W ( self ): self . X_shifted = _custom_roll ( self . X_bc , - 1 * self . s ) self . W_shifted = _custom_roll ( self . W_bc , - 1 * self . s ) def _fill_boundary_regions_V_bc ( self ): \"\"\"Extrapolate the edge values in V_bc over the extended boundaries\"\"\" V_filled = np . zeros_like ( self . V_bc ) idx = np . arange ( self . T + 2 * self . Ns ) for i , v in enumerate ( self . V_bc . T ): v_left = v [ idx <= int ( self . T / 2 )] v_right = v [ idx > int ( self . T / 2 )] v_left [ v_left == 0 ] = v_left [ np . argmax ( v_left != 0 )] v_right [ v_right == 0 ] = v_right [ np . argmax ( np . cumsum ( v_right != 0 ))] V_filled [:, i ] = np . concatenate ([ v_left , v_right ]) self . V_bc = V_filled def _update_V ( self ): V = tf . Variable ( self . V_bc , dtype = tf . float32 ) # @tf.function def _loss_V (): frob_tensor = tf . multiply ( self . W_shifted , self . X_shifted - ( self . U @ tf . transpose ( V )) ) frob_loss = tf . reduce_sum ( frob_tensor ** 2 ) l2_loss = self . config . lambda2 * tf . reduce_sum (( V - self . J ) ** 2 ) conv_loss = self . config . lambda3 * tf . reduce_sum ( ( tf . matmul ( self . KD , V ) ** 2 ) ) return frob_loss + l2_loss + conv_loss optimiser = tf . keras . optimizers . Adam ( learning_rate = self . config . learning_rate ) for _ in tf . range ( self . config . iter_V ): optimiser . minimize ( _loss_V , [ V ]) self . V_bc = V . numpy () def _exactly_solve_U ( self ): \"\"\"Solve for U at a fixed V. The internal U member is not modified by this method. V is assumed to be initialized.\"\"\" U = np . empty (( self . N , self . config . rank )) for n in range ( self . N ): U [ n ] = ( self . X_shifted [ n ] @ self . V_bc @ np . linalg . inv ( self . V_bc . T @ ( np . diag ( self . W_shifted [ n ]) @ self . V_bc ) + self . I1 ) ) return U def _approx_U ( self ): U = tf . Variable ( self . U , dtype = tf . float32 ) # @tf.function def _loss_U (): frob_tensor = tf . multiply ( self . W_shifted , self . X_shifted - tf . matmul ( U , self . V_bc , transpose_b = True ), ) frob_loss = tf . reduce_sum (( frob_tensor ) ** 2 ) return frob_loss + self . config . lambda1 * tf . reduce_sum ( U ** 2 ) optimiser = tf . keras . optimizers . Adam ( learning_rate = self . config . learning_rate ) for _ in tf . range ( self . config . iter_U ): optimiser . minimize ( _loss_U , [ U ]) return U . numpy () def _update_s ( self ): # Evaluate the discrepancy term for all possible shift candidates M = self . U @ self . V_bc . T D = ( np . linalg . norm ( self . W_shifts * ( self . X_shifts - M [ None , :, :]), axis =- 1 ) ** 2 ) # Selected shifts maximize the correlation between X and M s_new = [ self . config . shift_budget [ i ] for i in np . argmin ( D , axis = 0 )] # Update attributes only if changes to the optimal shift if not np . array_equal ( self . s , s_new ): self . s = np . array ( s_new ) self . _shift_X_W () def run_step ( self ): \"Perform one step of alternating minimization\" self . U = self . _approx_U () self . _update_V () self . _update_s () self . n_iter_ += 1 def loss ( self ): \"Compute the loss from the optimization objective\" loss = np . sum ( np . linalg . norm ( self . W_shifted * ( self . X_shifted - self . U @ self . V_bc . T ), axis = 1 ) ** 2 ) loss += self . config . lambda1 * np . sum ( np . linalg . norm ( self . U , axis = 1 ) ** 2 ) loss += self . config . lambda2 * np . linalg . norm ( self . V_bc ) ** 2 loss += self . config . lambda3 * np . linalg . norm ( self . KD @ self . V_bc ) ** 2 return loss","title":"SCMF"},{"location":"matfact/#matfact.model.factorization.factorizers.scmf.SCMF.V","text":"To be compatible with the expectation of having a V","title":"V"},{"location":"matfact/#matfact.model.factorization.WCMF","text":"Bases: BaseMF Matrix factorization with L2 and convolutional regularization. Factor updates are based on gradient descent approximations, permitting an arbitrary weight matrix in the discrepancy term. Parameters: Name Type Description Default X Sparse data matrix used to estimate factor matrices required V Initial estimate for basic vectors required config ModelConfig Configuration model. shift is ignored in the WCMF factorizer. required Source code in matfact/model/factorization/factorizers/wcmf.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 class WCMF ( BaseMF ): \"\"\"Matrix factorization with L2 and convolutional regularization. Factor updates are based on gradient descent approximations, permitting an arbitrary weight matrix in the discrepancy term. Args: X: Sparse data matrix used to estimate factor matrices V: Initial estimate for basic vectors config: Configuration model. shift is ignored in the WCMF factorizer. \"\"\" def __init__ ( self , X , config : ModelConfig , ): if config . shift_budget : warn ( \"WCMF given a non-empty shift budget. This will be ignored.\" \"Consider using SCMF.\" ) self . config = config self . X = X self . V = config . initial_basic_profiles_getter ( X . shape [ 1 ], config . rank ) self . W = self . config . weight_matrix_getter ( X ) self . N , self . T = np . shape ( self . X ) self . nz_rows , self . nz_cols = np . nonzero ( self . X ) self . n_iter_ = 0 KD = self . config . difference_matrix_getter ( self . T ) self . J = self . config . minimal_value_matrix_getter (( self . T , self . config . rank )) self . _init_matrices ( KD ) self . U = self . _exactly_solve_U () @property def M ( self ): return np . array ( self . U @ self . V . T , dtype = np . float32 ) def _init_matrices ( self , KD ): self . KD = tf . cast ( KD , dtype = tf . float32 ) self . DTKTKD = KD . T @ KD self . I_l1 = self . config . lambda1 * np . eye ( self . config . rank ) def _update_V ( self ): # @tf.function def _loss_V (): frob_tensor = tf . multiply ( W , X - ( U @ tf . transpose ( V ))) frob_loss = tf . square ( tf . norm ( frob_tensor )) l2_loss = self . config . lambda2 * tf . square ( tf . norm ( V - J )) conv_loss = self . config . lambda3 * tf . square ( tf . norm ( tf . matmul ( self . KD , V ))) return frob_loss + l2_loss + conv_loss V = tf . Variable ( self . V , dtype = tf . float32 ) J = tf . ones_like ( self . V , dtype = tf . float32 ) W = tf . cast ( self . W , dtype = tf . float32 ) X = tf . cast ( self . X , dtype = tf . float32 ) U = tf . cast ( self . U , dtype = tf . float32 ) optimiser = tf . keras . optimizers . Adam ( learning_rate = self . config . learning_rate ) for _ in tf . range ( self . config . iter_V ): optimiser . minimize ( _loss_V , [ V ]) self . V = V . numpy () def _exactly_solve_U ( self ): \"\"\"Solve for U at a fixed V. The internal U member is not modified by this method. V is assumed to be initialized.\"\"\" U = np . empty (( self . N , self . config . rank )) for n in range ( self . N ): U [ n ] = ( self . V . T @ ( self . W [ n ] * self . X [ n ]) @ np . linalg . inv ( self . V . T @ ( self . W [ n ][:, None ] * self . V ) + self . I_l1 ) ) return U def _approx_U ( self ): # @tf.function def _loss_U (): frob_tensor = tf . multiply ( W , X - tf . matmul ( U , V , transpose_b = True )) frob_loss = tf . square ( tf . norm ( frob_tensor )) return frob_loss + self . config . lambda1 * tf . square ( tf . norm ( U )) U = tf . Variable ( self . U , dtype = tf . float32 ) W = tf . cast ( self . W , dtype = tf . float32 ) X = tf . cast ( self . X , dtype = tf . float32 ) V = tf . cast ( self . V , dtype = tf . float32 ) optimiser = tf . keras . optimizers . Adam ( learning_rate = self . config . learning_rate ) for _ in tf . range ( self . config . iter_U ): optimiser . minimize ( _loss_U , [ U ]) return U . numpy () def loss ( self ): \"Compute the loss from the optimization objective\" loss = np . square ( np . linalg . norm ( self . W * ( self . X - self . U @ self . V . T ))) loss += self . config . lambda1 * np . square ( np . linalg . norm ( self . U )) loss += self . config . lambda2 * np . square ( np . linalg . norm ( self . V - 1 )) loss += self . config . lambda3 * np . square ( np . linalg . norm ( self . KD @ self . V )) return loss def run_step ( self ): \"Perform one step of alternating minimization\" self . U = self . _approx_U () self . _update_V () self . n_iter_ += 1","title":"WCMF"},{"location":"matfact/#matfact.model.factorization.convergence.ConvergenceMonitor","text":"Epoch generator with eager termination when converged. The model is said to have converged when the model's latent matrix (M) has a relative norm difference smaller than tolerance. Parameters: Name Type Description Default number_of_epochs int the maximum number of epochs to generate. DEFAULT_NUMBER_OF_EPOCHS epochs_per_val int convergence checking is done every epochs_per_val epoch. DEFAULT_EPOCHS_PER_VAL patience int the minimum number of epcohs. DEFAULT_PATIENCE show_progress bool enable tqdm progress bar. True tolerance float the tolerance under which the model is said to have converged. 0.0001 Examples: monitor = ConvergenceMonitor ( tolerance = 1e-5 ) for epoch in monitor ( model ): # If model converges, the generator will deplete before the default number # of epochs has been reached. ... Source code in matfact/model/factorization/convergence.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class ConvergenceMonitor : \"\"\"Epoch generator with eager termination when converged. The model is said to have converged when the model's latent matrix (M) has a relative norm difference smaller than tolerance. Args: number_of_epochs: the maximum number of epochs to generate. epochs_per_val: convergence checking is done every epochs_per_val epoch. patience: the minimum number of epcohs. show_progress: enable tqdm progress bar. tolerance: the tolerance under which the model is said to have converged. Examples: ```python monitor = ConvergenceMonitor(tolerance=1e-5) for epoch in monitor(model): # If model converges, the generator will deplete before the default number # of epochs has been reached. ... ``` \"\"\" def __init__ ( self , number_of_epochs : int = DEFAULT_NUMBER_OF_EPOCHS , epochs_per_val : int = DEFAULT_EPOCHS_PER_VAL , patience : int = DEFAULT_PATIENCE , show_progress : bool = True , tolerance : float = 1e-4 , ): self . number_of_epochs = number_of_epochs self . tolerance = tolerance self . epochs_per_val = epochs_per_val self . patience = patience self . _range = trange if show_progress else range @staticmethod def _difference_func ( new_M , old_M ): return np . sum (( new_M - old_M ) ** 2 ) / np . sum ( old_M ** 2 ) def __call__ ( self , model : \"BaseMF\" ): \"\"\"A generator that yields epoch numbers.\"\"\" _old_M = model . M _model = model # mypy does not recognize the union of trange and range as a callable. for i in self . _range ( self . number_of_epochs ): # type: ignore yield i # model is expected to update its M should_update = i > self . patience and i % self . epochs_per_val == 0 if should_update : if self . _difference_func ( _model . M , _old_M ) < self . tolerance : break _old_M = model . M","title":"ConvergenceMonitor"},{"location":"matfact/#matfact.model.factorization.convergence.ConvergenceMonitor.__call__","text":"A generator that yields epoch numbers. Source code in matfact/model/factorization/convergence.py 64 65 66 67 68 69 70 71 72 73 74 75 def __call__ ( self , model : \"BaseMF\" ): \"\"\"A generator that yields epoch numbers.\"\"\" _old_M = model . M _model = model # mypy does not recognize the union of trange and range as a callable. for i in self . _range ( self . number_of_epochs ): # type: ignore yield i # model is expected to update its M should_update = i > self . patience and i % self . epochs_per_val == 0 if should_update : if self . _difference_func ( _model . M , _old_M ) < self . tolerance : break _old_M = model . M","title":"__call__()"},{"location":"matfact/#matfact.model.logging","text":"Logging utilities. The logging is written in a general way, however at the moment only MLFlow is supported as backend.","title":"logging"},{"location":"matfact/#matfact.model.logging.MLFlowBatchLogger","text":"Bases: MLFlowLogger Context manager for combining multiple run data into one MLFlow run. Given several run dictionaries, the data is aggregated together to one summary run, which is logged to MLFlow. Used in for example cross validation runs, where each fold is a subrun, and the entire cross validation is logged as one run. Examples: It is possible to run MLFlowBatchLogger wrapped around subrun contexts. >>> with MLFlowBatchLogger () as outer_logger : >>> for subrun in subruns : >>> with MLFlowLogger () as inner_logger : >>> ... >>> inner_logger ( run_data ) >>> outer_logger ( run_data ) # (1) The data is only logged to MLFlow when MLFlowBatchLogger exists. How and when are things logged? Each call to the logger is stored as a subrun, which on exit is aggregated together to one run which is logged to MLFLow. See matfact.model.logging.batch_mlflow_logger for details. Source code in matfact/model/logging.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 class MLFlowBatchLogger ( MLFlowLogger ): \"\"\"Context manager for combining multiple run data into one MLFlow run. Given several run dictionaries, the data is aggregated together to one summary run, which is logged to MLFlow. Used in for example cross validation runs, where each fold is a subrun, and the entire cross validation is logged as one run. Examples: It is possible to run MLFlowBatchLogger wrapped around subrun contexts. >>> with MLFlowBatchLogger() as outer_logger: >>> for subrun in subruns: >>> with MLFlowLogger() as inner_logger: >>> ... >>> inner_logger(run_data) >>> outer_logger(run_data) # (1) 1. The data is only logged to MLFlow when `MLFlowBatchLogger` exists. ??? Note \"How and when are things logged?\" Each call to the logger is stored as a subrun, which on exit is aggregated together to one run which is logged to MLFLow. See [matfact.model.logging.batch_mlflow_logger][] for details. \"\"\" def __init__ ( self , allow_nesting : bool = True , extra_tags : dict | None = None , aggregate_funcs : list [ AggregationFunction ] | None = None , ) -> None : super () . __init__ ( allow_nesting = allow_nesting , extra_tags = extra_tags ) self . aggregate_funcs = aggregate_funcs def __enter__ ( self ): self . output = [] return super () . __enter__ () def __exit__ ( self , type , value , traceback ): batch_mlflow_logger ( self . output , aggregate_funcs = self . aggregate_funcs ) return super () . __exit__ ( type , value , traceback ) def __call__ ( self , output_dict ): # On call we only append the dict to our list of data. # The actual logging happens on __exit__. self . output . append ( output_dict ) if self . extra_tags : mlflow . set_tags ( self . extra_tags )","title":"MLFlowBatchLogger"},{"location":"matfact/#matfact.model.logging.MLFlowLogger","text":"Context manager for MLFlow logging. Wraps the code inside the corresponding with block in an MLFlow run. Parameters: Name Type Description Default allow_nesting bool loggers can be nested within each other, with inside runs being logged as children in MLFlow. True extra_tags dict | None these tags will be appended to each run. None Examples: with MLFlowLogger () as logger : output = get_output_data () # output = { # \"params\": {...}, # \"metrics\": {...}, # \"meta\": {...}, # } logger ( output ) Raises: Type Description MLFlowRunHierarchyException Raised on enter if loggers are nested when allow_nesting is False. See also MLFlowBatchLogger MLFlowLoggerArtifact MLFlowLoggerDiagnostic Source code in matfact/model/logging.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 class MLFlowLogger : \"\"\"Context manager for MLFlow logging. Wraps the code inside the corresponding with block in an MLFlow run. Arguments: allow_nesting: loggers can be nested within each other, with inside runs being logged as children in MLFlow. extra_tags: these tags will be appended to each run. Examples: ```python with MLFlowLogger() as logger: output = get_output_data() # output = { # \"params\": {...}, # \"metrics\": {...}, # \"meta\": {...}, # } logger(output) ``` Raises: MLFlowRunHierarchyException: Raised on enter if loggers are nested when allow_nesting is False. See also: - [MLFlowBatchLogger][matfact.model.logging.MLFlowBatchLogger] - [MLFlowLoggerArtifact][matfact.model.logging.MLFlowLoggerArtifact] - [MLFlowLoggerDiagnostic][matfact.model.logging.MLFlowLoggerDiagnostic] \"\"\" def __init__ ( self , allow_nesting : bool = True , extra_tags : dict | None = None ): self . allow_nesting = allow_nesting self . extra_tags = extra_tags if extra_tags else {} def __enter__ ( self ): try : self . run_ = mlflow . start_run ( nested = self . allow_nesting ) except Exception as e : if re . match ( \"Run with UUID [0-9a-f]+ is already active.\" , str ( e )): self . __exit__ ( type ( e ), str ( e ), e . __traceback__ ) raise MLFlowRunHierarchyException ( \"allow_nesting is False, but loggers are nested!\" ) return self def __exit__ ( self , type , value , traceback ): \"\"\"End the run by calling the underlying ActiveRun's exit method.\"\"\" if hasattr ( self , \"run_\" ): return self . run_ . __exit__ ( type , value , traceback ) else : return True def __call__ ( self , output_dict : dict ): \"\"\"Log an output dict to MLFlow.\"\"\" mlflow_logger ( output_dict ) mlflow . set_tags ( self . extra_tags )","title":"MLFlowLogger"},{"location":"matfact/#matfact.model.logging.MLFlowLoggerArtifact","text":"Bases: MLFlowLogger Context manager for MLFlow logging, with artifact logging. All artifacts in artifact_path will be logged to the MLFlow run. Source code in matfact/model/logging.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 class MLFlowLoggerArtifact ( MLFlowLogger ): \"\"\"Context manager for MLFlow logging, with artifact logging. All artifacts in artifact_path will be logged to the MLFlow run.\"\"\" def __init__ ( self , artifact_path : pathlib . Path , allow_nesting : bool = True , extra_tags : dict | None = None , create_artifact_path : bool = settings . create_path_default , ): super () . __init__ ( allow_nesting = allow_nesting , extra_tags = extra_tags ) self . figure_path = artifact_path if create_artifact_path : artifact_path . mkdir ( parents = True , exist_ok = True ) def __call__ ( self , output_dict ): super () . __call__ ( output_dict ) mlflow . log_artifacts ( self . figure_path )","title":"MLFlowLoggerArtifact"},{"location":"matfact/#matfact.model.logging.MLFlowLoggerDiagnostic","text":"Bases: MLFlowLoggerArtifact Context manager for MLFlow logging, generating default diagnostic plots. Source code in matfact/model/logging.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 class MLFlowLoggerDiagnostic ( MLFlowLoggerArtifact ): \"\"\"Context manager for MLFlow logging, generating default diagnostic plots.\"\"\" def __call__ ( self , output_dict ): solver_output = output_dict [ \"meta\" ][ \"results\" ] plot_coefs ( solver_output [ \"U\" ], self . figure_path ) plot_basis ( solver_output [ \"V\" ], self . figure_path ) number_of_states = solver_output [ \"p_pred\" ] . shape [ 1 ] plot_confusion ( solver_output [ \"x_true\" ], solver_output [ \"x_pred\" ], self . figure_path , n_classes = number_of_states , ) plot_roc_curve ( solver_output [ \"x_true\" ], solver_output [ \"p_pred\" ], self . figure_path , number_of_states = number_of_states , ) plot_certainty ( solver_output [ \"p_pred\" ], solver_output [ \"x_true\" ], self . figure_path ) super () . __call__ ( output_dict )","title":"MLFlowLoggerDiagnostic"},{"location":"matfact/#matfact.model.logging.batch_mlflow_logger","text":"Combine and log a set of runs. Used in for example cross validation training, where all folds should be logged as one run. Parameters: Name Type Description Default log_data list [ dict ] list of run data. Each entry in log_data should be compatible with the format expected by _mlflow_logger . { \"params\" : { \"field1\" : value1 ,}, \"metrics\" : { \"field2\" : foo , \"field_history\" : [ ... ],}, \"tags\" : {}, } required Source code in matfact/model/logging.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def batch_mlflow_logger ( log_data : list [ dict ], aggregate_funcs : list [ AggregationFunction ] | None = None ) -> None : \"\"\"Combine and log a set of runs. Used in for example cross validation training, where all folds should be logged as one run. Arguments: log_data: list of run data. Each entry in log_data should be compatible with the format expected by `_mlflow_logger`. ```python { \"params\": {\"field1\": value1,}, \"metrics\": {\"field2\": foo, \"field_history\": [...],}, \"tags\": {}, } ``` \"\"\" new_log : dict = { \"params\" : {}, \"metrics\" : {}, \"tags\" : {}, } new_log [ \"params\" ] = _aggregate_fields ( [ data [ \"params\" ] for data in log_data ], aggregate_funcs = aggregate_funcs ) new_log [ \"metrics\" ] = _aggregate_fields ( [ data [ \"metrics\" ] for data in log_data ], aggregate_funcs = aggregate_funcs ) mlflow_logger ( new_log )","title":"batch_mlflow_logger()"},{"location":"matfact/#matfact.model.logging.mlflow_logger","text":"Log results dictionary to MLFlow. Given a dictionary on the format below, add the run to mlflow. Assumes there to be an active MLFlow run! The run is typically started by MLFLowLogger . Params and metrics should have values that are floats. Metric also accepts a list of floats, in which case they are interpreted as the metric value as a function of the epochs. { \"params\" : { \"param1\" : value , \"param2\" : value ,}, \"metrics\" : { \"metric1\" : value , \"metric2\" : [ ... ]}, \"tags\" : {}, \"meta\" : {}, # Data not logged to MLFLow } Source code in matfact/model/logging.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def mlflow_logger ( log_data : dict ) -> None : \"\"\"Log results dictionary to MLFlow. Given a dictionary on the format below, add the run to mlflow. Assumes there to be an active MLFlow run! The run is typically started by [`MLFLowLogger`][matfact.model.logging.MLFlowLogger]. Params and metrics should have values that are floats. Metric also accepts a list of floats, in which case they are interpreted as the metric value as a function of the epochs. ```python { \"params\": {\"param1\": value, \"param2\": value,}, \"metrics\": {\"metric1\": value, \"metric2\": [...]}, \"tags\": {}, \"meta\": {}, # Data not logged to MLFLow } ``` \"\"\" for parameter , value in log_data [ \"params\" ] . items (): mlflow . log_param ( parameter , value ) for metric , value in log_data [ \"metrics\" ] . items (): if isinstance ( value , list ): for epoch , value_at_epoch in enumerate ( value ): mlflow . log_metric ( metric , value_at_epoch , step = epoch ) else : mlflow . log_metric ( metric , value ) mlflow . set_tags ( log_data [ \"tags\" ])","title":"mlflow_logger()"},{"location":"matfact/#matfact.data_generation.Dataset","text":"Screening dataset container This class simplifies generating, loading, and saving datasets. Chaining Most methods returns the Dataset object, so that it is chainable, as Dataset . from_file ( some_path ) . get_X_M () Source code in matfact/data_generation/dataset.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 class Dataset : \"\"\"Screening dataset container This class simplifies generating, loading, and saving datasets. !!! Note \"Chaining\" Most methods returns the Dataset object, so that it is chainable, as ```python Dataset.from_file(some_path).get_X_M() ``` \"\"\" def __init__ ( self , X : np . ndarray , M : np . ndarray , metadata : dict ): self . X = X self . M = M self . metadata = metadata def __str__ ( self ): return ( \"Dataset object of size\" f \" { self . metadata [ 'N' ] } x { self . metadata [ 'T' ] } with rank\" f \" { self . metadata [ 'rank' ] } and sparsity level\" f \" { self . metadata [ 'sparsity_level' ] } \" ) @classmethod def from_file ( cls , path : pathlib . Path ): \"\"\"Load dataset from file\"\"\" X , M = np . load ( path / \"X.npy\" ), np . load ( path / \"M.npy\" ) with ( path / \"dataset_metadata.json\" ) . open ( \"r\" ) as metadata_file : metadata = json . load ( metadata_file ) return cls ( X , M , metadata ) def save ( self , path : pathlib . Path ): \"\"\"Store dataset to file\"\"\" np . save ( path / \"X.npy\" , self . X ) np . save ( path / \"M.npy\" , self . M ) with ( path / \"dataset_metadata.json\" ) . open ( \"w\" ) as metadata_file : metadata_file . write ( json . dumps ( self . metadata )) return self @classmethod def generate ( cls , N , T , rank , sparsity_level , produce_dataset_function = produce_dataset , number_of_states = default_number_of_states , observation_probabilities = default_observation_probabilities , ): \"\"\"Generate a Dataset produce_dataset_function should be a callable with signature ``` Callable( N, T, rank, sparsity_level, *, number_of_states, observation_probabilities ) -> observed_matrix: ndarray, latent_matrix: ndarray, generation_name: str ``` \"\"\" X , M , generation_name = produce_dataset_function ( N , T , rank , sparsity_level , number_of_states = number_of_states , observation_probabilities = observation_probabilities , ) number_of_individuals = X . shape [ 0 ] if number_of_individuals == 0 : raise RuntimeError ( \"Data generation produced no valid screening data!\" ) metadata = { \"rank\" : rank , \"sparsity_level\" : sparsity_level , \"N\" : N , \"T\" : T , \"generation_method\" : generation_name , \"number_of_states\" : number_of_states , \"observation_probabilities\" : list ( observation_probabilities ), } return cls ( X , M , metadata ) def get_X_M ( self ): \"\"\"Return the X and M matrix.\"\"\" return self . X , self . M def get_split_X_M ( self , ratio = 0.8 ): \"\"\"Split dataset into train and test subsets.\"\"\" X , M = self . get_X_M () slice_index = int ( X . shape [ 0 ] * ratio ) return X [: slice_index ], X [ slice_index :], M [: slice_index ], M [ slice_index :] def prefixed_metadata ( self , prefix = \"DATASET_\" ): \"\"\"Return the metadata dict with prefix prepended to keys Convenience method used in for example logging\"\"\" return { prefix + key : value for key , value in self . metadata . items ()}","title":"Dataset"},{"location":"matfact/#matfact.data_generation.dataset.Dataset.from_file","text":"Load dataset from file Source code in matfact/data_generation/dataset.py 116 117 118 119 120 121 122 @classmethod def from_file ( cls , path : pathlib . Path ): \"\"\"Load dataset from file\"\"\" X , M = np . load ( path / \"X.npy\" ), np . load ( path / \"M.npy\" ) with ( path / \"dataset_metadata.json\" ) . open ( \"r\" ) as metadata_file : metadata = json . load ( metadata_file ) return cls ( X , M , metadata )","title":"from_file()"},{"location":"matfact/#matfact.data_generation.dataset.Dataset.generate","text":"Generate a Dataset produce_dataset_function should be a callable with signature Callable( N, T, rank, sparsity_level, *, number_of_states, observation_probabilities ) -> observed_matrix: ndarray, latent_matrix: ndarray, generation_name: str Source code in matfact/data_generation/dataset.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 @classmethod def generate ( cls , N , T , rank , sparsity_level , produce_dataset_function = produce_dataset , number_of_states = default_number_of_states , observation_probabilities = default_observation_probabilities , ): \"\"\"Generate a Dataset produce_dataset_function should be a callable with signature ``` Callable( N, T, rank, sparsity_level, *, number_of_states, observation_probabilities ) -> observed_matrix: ndarray, latent_matrix: ndarray, generation_name: str ``` \"\"\" X , M , generation_name = produce_dataset_function ( N , T , rank , sparsity_level , number_of_states = number_of_states , observation_probabilities = observation_probabilities , ) number_of_individuals = X . shape [ 0 ] if number_of_individuals == 0 : raise RuntimeError ( \"Data generation produced no valid screening data!\" ) metadata = { \"rank\" : rank , \"sparsity_level\" : sparsity_level , \"N\" : N , \"T\" : T , \"generation_method\" : generation_name , \"number_of_states\" : number_of_states , \"observation_probabilities\" : list ( observation_probabilities ), } return cls ( X , M , metadata )","title":"generate()"},{"location":"matfact/#matfact.data_generation.dataset.Dataset.get_X_M","text":"Return the X and M matrix. Source code in matfact/data_generation/dataset.py 176 177 178 def get_X_M ( self ): \"\"\"Return the X and M matrix.\"\"\" return self . X , self . M","title":"get_X_M()"},{"location":"matfact/#matfact.data_generation.dataset.Dataset.get_split_X_M","text":"Split dataset into train and test subsets. Source code in matfact/data_generation/dataset.py 180 181 182 183 184 def get_split_X_M ( self , ratio = 0.8 ): \"\"\"Split dataset into train and test subsets.\"\"\" X , M = self . get_X_M () slice_index = int ( X . shape [ 0 ] * ratio ) return X [: slice_index ], X [ slice_index :], M [: slice_index ], M [ slice_index :]","title":"get_split_X_M()"},{"location":"matfact/#matfact.data_generation.dataset.Dataset.prefixed_metadata","text":"Return the metadata dict with prefix prepended to keys Convenience method used in for example logging Source code in matfact/data_generation/dataset.py 186 187 188 189 190 def prefixed_metadata ( self , prefix = \"DATASET_\" ): \"\"\"Return the metadata dict with prefix prepended to keys Convenience method used in for example logging\"\"\" return { prefix + key : value for key , value in self . metadata . items ()}","title":"prefixed_metadata()"},{"location":"matfact/#matfact.data_generation.dataset.Dataset.save","text":"Store dataset to file Source code in matfact/data_generation/dataset.py 124 125 126 127 128 129 130 def save ( self , path : pathlib . Path ): \"\"\"Store dataset to file\"\"\" np . save ( path / \"X.npy\" , self . X ) np . save ( path / \"M.npy\" , self . M ) with ( path / \"dataset_metadata.json\" ) . open ( \"w\" ) as metadata_file : metadata_file . write ( json . dumps ( self . metadata )) return self Utility functions for masking prediction data used in training.","title":"save()"},{"location":"matfact/#matfact.model.predict.dataset_utils.prediction_data","text":"Mask observed data to produce test data for predictions. Parameters: Name Type Description Default prediction_time_strategy Callable [[ npt . NDArray ], npt . NDArray ] Callable returning prediction times given the observation_matrix. Defaults to choosing the last observation. _last_observed_time row_masking_strategy Callable [[ npt . NDArray , int ], None] Callable masking an observation row, i.e. individual history, given a time point. Note that the masking is done in place, not by return. Defaults to masking the three time slots from prediction time and back. _mask_row Examples: >>> masked_observation , times , correct_values = prediction_data ( my_data ) >>> my_data [ range ( times . size ), times ] == correct_values True >>> ( masked_observation [ range ( times . size ), times ] == 0 ) . all () True Source code in matfact/model/predict/dataset_utils.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def prediction_data ( observation_matrix : npt . NDArray , prediction_time_strategy : Callable [ [ npt . NDArray ], npt . NDArray ] = _last_observed_time , row_masking_strategy : Callable [[ npt . NDArray , int ], None ] = _mask_row , ) -> tuple [ npt . NDArray , npt . NDArray , npt . NDArray ]: \"\"\"Mask observed data to produce test data for predictions. Arguments: prediction_time_strategy: Callable returning prediction times given the observation_matrix. Defaults to choosing the last observation. row_masking_strategy: Callable masking an observation row, i.e. individual history, given a time point. Note that the masking is done in place, not by return. Defaults to masking the three time slots from prediction time and back. Examples: >>> masked_observation, times, correct_values = prediction_data(my_data) >>> my_data[range(times.size), times] == correct_values True >>> (masked_observation[range(times.size), times] == 0).all() True \"\"\" times = prediction_time_strategy ( observation_matrix ) # Choose the value at times for each row correct_values = np . take_along_axis ( observation_matrix , times [:, None ], axis = 1 ) . flatten () # row_masking_strategy masks in-place, so important to copy! masked_observation_matrix = np . copy ( observation_matrix ) for i , row in enumerate ( masked_observation_matrix ): row_masking_strategy ( row , times [ i ]) return masked_observation_matrix , times , correct_values Threshold based classification. To correct for the data being skewed, we introduce some threshold values that favor less likely classes, even when they do not have the highest probability. Consider we have some probabilities [p0, p1, p2], where pi is the probability of class i. We introduce thresholds [t1, t2], such that instead of choosing the class with the highest probability, we choose the class as follows: beginning from the class with the highest number (assumed to be the rearest), check if p2 > t2. If so, we set the class to p2. Next, check p1 > t1, etc. In general, given probabilities [p0, p1, p2, ...] and thresholds [t1, t2, ...], set the class to max(i) where pi > ti. This prediction algorithm is implemented in ClassificationTree . In addition, estimate_probability_thresholds estimates the optimal values of the thresholds.","title":"prediction_data()"},{"location":"matfact/#matfact.model.predict.classification_tree.ClassificationTree","text":"Bases: BaseEstimator , ClassifierMixin Perform hierarchical classification given probability thresholds. Class labels are 1 indexed integers. The number of thresholds (tau) is one less than the number of classes. Source code in matfact/model/predict/classification_tree.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class ClassificationTree ( BaseEstimator , ClassifierMixin ): \"\"\"Perform hierarchical classification given probability thresholds. Class labels are 1 indexed integers. The number of thresholds (tau) is one less than the number of classes. \"\"\" def __init__ ( self , thresholds : Sequence [ float ] | None = None ): self . thresholds = thresholds if thresholds is not None else [] def predict ( self , probabilities : np . ndarray ): \"\"\"Perform classification given probabilities for classes. Arguments: probabilities: (number_of_samples x number_of_classes) ndarray \"\"\" number_of_samples , number_of_classes = probabilities . shape if number_of_classes != len ( self . thresholds ) + 1 : raise ValueError ( f \"Probabilities for { number_of_classes } classes given. \" \"The number of thresholds should be one less than the number of classes\" f \", but it is { len ( self . thresholds ) } .\" ) # Set all samples to class one. # Iterate through the classes, if the probability is above the # threshold, assign that class. classes = np . ones ( number_of_samples ) for i , threshold in enumerate ( self . thresholds ): # Threshold i correspond to class i + 1, so add one classes [ probabilities [:, i + 1 ] >= threshold ] = i + 1 return classes def fit ( self , X : Any , y : Any ): \"\"\"Do nothing, fit required by sklearn API specification.\"\"\" return self","title":"ClassificationTree"},{"location":"matfact/#matfact.model.predict.classification_tree.ClassificationTree.fit","text":"Do nothing, fit required by sklearn API specification. Source code in matfact/model/predict/classification_tree.py 62 63 64 def fit ( self , X : Any , y : Any ): \"\"\"Do nothing, fit required by sklearn API specification.\"\"\" return self","title":"fit()"},{"location":"matfact/#matfact.model.predict.classification_tree.ClassificationTree.predict","text":"Perform classification given probabilities for classes. probabilities: (number_of_samples x number_of_classes) ndarray Source code in matfact/model/predict/classification_tree.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def predict ( self , probabilities : np . ndarray ): \"\"\"Perform classification given probabilities for classes. Arguments: probabilities: (number_of_samples x number_of_classes) ndarray \"\"\" number_of_samples , number_of_classes = probabilities . shape if number_of_classes != len ( self . thresholds ) + 1 : raise ValueError ( f \"Probabilities for { number_of_classes } classes given. \" \"The number of thresholds should be one less than the number of classes\" f \", but it is { len ( self . thresholds ) } .\" ) # Set all samples to class one. # Iterate through the classes, if the probability is above the # threshold, assign that class. classes = np . ones ( number_of_samples ) for i , threshold in enumerate ( self . thresholds ): # Threshold i correspond to class i + 1, so add one classes [ probabilities [:, i + 1 ] >= threshold ] = i + 1 return classes","title":"predict()"},{"location":"matfact/#matfact.model.predict.classification_tree.estimate_probability_thresholds","text":"Estimate threshold values for ClassificationTree with differential evolution. Parameters: Name Type Description Default y_true np . ndarray Vector of class labels. required y_pred_proba Vector of predicted probabilities. required Returns: Type Description A ClassificationTree object instantiated with the estimated probability thresholds. This object may be saved to disk using scikit-learn routines. Source code in matfact/model/predict/classification_tree.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def estimate_probability_thresholds ( y_true : np . ndarray , y_predicted_probabilities : np . ndarray , tol : float = 1e-6 , seed : int = 42 , ): \"\"\"Estimate threshold values for ClassificationTree with differential evolution. Args: y_true: Vector of class labels. y_pred_proba: Vector of predicted probabilities. Returns: A ClassificationTree object instantiated with the estimated probability thresholds. This object may be saved to disk using scikit-learn routines. \"\"\" check_X_y ( y_predicted_probabilities , y_true ) number_of_classes = y_predicted_probabilities . shape [ 1 ] result = optimize . differential_evolution ( _matthews_correlation_coefficient_objective , # Bounds are [0, 1] for each threshold value, i.e. one less than the number # of classes. Iterators are not accepted, so convert to list. bounds = list ( itertools . repeat (( 0 , 1 ), number_of_classes - 1 )), args = ( y_true , y_predicted_probabilities , ClassificationTree ()), seed = seed , tol = tol , ) return ClassificationTree ( thresholds = result . x )","title":"estimate_probability_thresholds()"},{"location":"navigation/","text":"Overview Usage Reference MatFact HMM Examples examples/*","title":"Navigation"},{"location":"usage/","text":"Usage Note These examples does not reflect what is implemented, but rather serves as a specification for what we want for the future. MatFact Ideal usage pattern: Without logging With logging from decipher.data import Dataset , prediction_data from matfact import MatFact , ModelConfig # Initialize model and dataset matfact = MatFact ( ModelConfig ( use_threshold_optimization = True )) dataset = Dataset . generate () # Split dataset into train and test X_train , X_test = dataset . get_split_X () # Fit matfact . fit ( X_train ) # (1) # Predict on masked out data X_test_masked , t_pred_test , x_true_test = prediction_data ( X_test ) x_prediciton_test = matfact . predict ( X_test_masked ) # Get score score = some_score_function ( x_true_test , x_prediction_test ) In fit, several things happen. A factorizer ( BaseMF ) is instantiated. The observed matrix ( X_train ) is completed. As threshold optimization is enabled, the classification thresholds are optimized. Probabilities for the different states are predicted for a masked sample of X_train . (Using prediction_data to mask the data and matfact.predict_probability ). The optimal classification thresholds are solved for, and a ClassificationTree with those thresholds is created. from decipher.logging import MLFlowLogger from decipher.data import Dataset , prediction_data from matfact import MatFact , ModelConfig # Initialize model and dataset matfact = MatFact ( ModelConfig ( use_threshold_optimization = True )) dataset = Dataset . generate () # Split dataset into train and test X_train , X_test = dataset . get_split_X () with MLFlowLogger () as logger : # (1) # Fit matfact . fit ( X_train ) # Predict on masked out data X_test_masked , t_pred_test , x_true_test = prediction_data ( X_test ) x_prediciton_test = matfact . predict ( X_test_masked ) # Get score score = some_score_function ( x_true_test , x_prediction_test ) # Log logger ( matfact . get_output_dict ()) Alternative prediction modules MatFact is designed such that it should be simple to experiment with alternative prediction modules. There are two stages to the prediction in matfact: generating probabilities for the classes, and choosing one class given such probabilities. These modules are easily modified. MatFact takes in a predictor and probability estimator . flowchart LR data -- <code>ProbabilityEstimator</code> --> probabilities probabilities -- <code>Predictor</code> --> state Replacing the predictor Using an alternative probability predictor from matfact import MatFact , ClassificationTreePredictor , ArgmaxPredictor from decipher.data import Dataset , prediction_data dataset = Dataset . generate () X_masked , t_pred , x_true = prediction_data ( dataset . X ) matfact_argmax = MatFact ( predictor = ArgmaxPredictor ()) matfact_clf = MatFact ( predictor = ClassificationTreePredictor ()) # The instances are otherwise used in the exact same way. matfact_argmax . fit ( X_masked ) . predict ( X_masked , t_pred ) matfact_clf . fit ( X_masked ) . predict ( X_masked , t_pred ) This is simply a mock demonstration, HPVProbabilityEstimator does not exist. class HPVProbabilityEstimator : def predict_probability ( self , matfact , observation_matrix , time_points ): return geneous_algorithm ( matfact , observation_matrix , time_points ) MatFact ( ModelConfig (), probability_estimator = HPVProbabilityEstimator ()) HMM Usage pattern: Data synthetization Prediction from decipher.data import Dataset from decipher.settings import Settings from hmm import HMM , ModelConfig hmm = HMM ( ModelConfig ()) # Use default transition weights, so no fit needed. dataset = Dataset . get_from_database () settings = Settings () synthetic_data : Dataset = hmm . genereate ( number_of_samples = settings . default_sample_number , # Number of individuals to simulate time_steps = settings . default_time_steps , # The number of time steps emissions = [ 4 , 2 ], # (1) ) Two emissions, the first with four possible states the second with two. This may for example correspond to one four-state exam and one binary HPV exam. Terminology inspired by Dynamax . from decipher.data import Dataset from decipher.settings import Settings from hmm import HMM , ModelConfig hmm = HMM ( ModelConfig ()) # Use default transition weights, so no fit needed. dataset = Dataset . get_from_database () X = dataset . get_X () # Predict on masked out data X_masked , t_pred , x_true = prediction_data ( X ) x_prediction = matfact . predict ( X_masked ) # Get score score = some_score_function ( x_true , x_prediction )","title":"Usage"},{"location":"usage/#usage","text":"Note These examples does not reflect what is implemented, but rather serves as a specification for what we want for the future.","title":"Usage"},{"location":"usage/#matfact","text":"Ideal usage pattern: Without logging With logging from decipher.data import Dataset , prediction_data from matfact import MatFact , ModelConfig # Initialize model and dataset matfact = MatFact ( ModelConfig ( use_threshold_optimization = True )) dataset = Dataset . generate () # Split dataset into train and test X_train , X_test = dataset . get_split_X () # Fit matfact . fit ( X_train ) # (1) # Predict on masked out data X_test_masked , t_pred_test , x_true_test = prediction_data ( X_test ) x_prediciton_test = matfact . predict ( X_test_masked ) # Get score score = some_score_function ( x_true_test , x_prediction_test ) In fit, several things happen. A factorizer ( BaseMF ) is instantiated. The observed matrix ( X_train ) is completed. As threshold optimization is enabled, the classification thresholds are optimized. Probabilities for the different states are predicted for a masked sample of X_train . (Using prediction_data to mask the data and matfact.predict_probability ). The optimal classification thresholds are solved for, and a ClassificationTree with those thresholds is created. from decipher.logging import MLFlowLogger from decipher.data import Dataset , prediction_data from matfact import MatFact , ModelConfig # Initialize model and dataset matfact = MatFact ( ModelConfig ( use_threshold_optimization = True )) dataset = Dataset . generate () # Split dataset into train and test X_train , X_test = dataset . get_split_X () with MLFlowLogger () as logger : # (1) # Fit matfact . fit ( X_train ) # Predict on masked out data X_test_masked , t_pred_test , x_true_test = prediction_data ( X_test ) x_prediciton_test = matfact . predict ( X_test_masked ) # Get score score = some_score_function ( x_true_test , x_prediction_test ) # Log logger ( matfact . get_output_dict ())","title":"MatFact"},{"location":"usage/#alternative-prediction-modules","text":"MatFact is designed such that it should be simple to experiment with alternative prediction modules. There are two stages to the prediction in matfact: generating probabilities for the classes, and choosing one class given such probabilities. These modules are easily modified. MatFact takes in a predictor and probability estimator . flowchart LR data -- <code>ProbabilityEstimator</code> --> probabilities probabilities -- <code>Predictor</code> --> state Replacing the predictor Using an alternative probability predictor from matfact import MatFact , ClassificationTreePredictor , ArgmaxPredictor from decipher.data import Dataset , prediction_data dataset = Dataset . generate () X_masked , t_pred , x_true = prediction_data ( dataset . X ) matfact_argmax = MatFact ( predictor = ArgmaxPredictor ()) matfact_clf = MatFact ( predictor = ClassificationTreePredictor ()) # The instances are otherwise used in the exact same way. matfact_argmax . fit ( X_masked ) . predict ( X_masked , t_pred ) matfact_clf . fit ( X_masked ) . predict ( X_masked , t_pred ) This is simply a mock demonstration, HPVProbabilityEstimator does not exist. class HPVProbabilityEstimator : def predict_probability ( self , matfact , observation_matrix , time_points ): return geneous_algorithm ( matfact , observation_matrix , time_points ) MatFact ( ModelConfig (), probability_estimator = HPVProbabilityEstimator ())","title":"Alternative prediction modules"},{"location":"usage/#hmm","text":"Usage pattern: Data synthetization Prediction from decipher.data import Dataset from decipher.settings import Settings from hmm import HMM , ModelConfig hmm = HMM ( ModelConfig ()) # Use default transition weights, so no fit needed. dataset = Dataset . get_from_database () settings = Settings () synthetic_data : Dataset = hmm . genereate ( number_of_samples = settings . default_sample_number , # Number of individuals to simulate time_steps = settings . default_time_steps , # The number of time steps emissions = [ 4 , 2 ], # (1) ) Two emissions, the first with four possible states the second with two. This may for example correspond to one four-state exam and one binary HPV exam. Terminology inspired by Dynamax . from decipher.data import Dataset from decipher.settings import Settings from hmm import HMM , ModelConfig hmm = HMM ( ModelConfig ()) # Use default transition weights, so no fit needed. dataset = Dataset . get_from_database () X = dataset . get_X () # Predict on masked out data X_masked , t_pred , x_true = prediction_data ( X ) x_prediction = matfact . predict ( X_masked ) # Get score score = some_score_function ( x_true , x_prediction )","title":"HMM"},{"location":"examples/example/","text":"Simple example # example.py import pathlib from itertools import product import matplotlib import mlflow import numpy as np import tensorflow as tf from sklearn.metrics import accuracy_score , matthews_corrcoef from matfact import settings from matfact.data_generation import Dataset from matfact.model import model_factory , prediction_data , reconstruction_mse from matfact.plotting import ( plot_basis , plot_certainty , plot_coefs , plot_confusion , plot_roc_curve , ) from matfact.settings import DATASET_PATH , FIGURE_PATH def experiment ( hyperparams , enable_shift : bool = False , enable_weighting : bool = False , enable_convolution : bool = False , mlflow_tags : dict = None , dataset_path : pathlib . Path = DATASET_PATH , ): \"\"\"Execute and log an experiment. Loads the dataset in dataset_path and splits this into train and test sets. The test set is masked, so that the last observation is hidden. The matrix completion is solved on the train set and then the probability of the possible states of the (masked) train set is comptued. The run is tracked using MLFLow, using several metrics and artifacts (files). Parameters: hyperparams: Dict of hyperparameters passed to the model. Common for all models is { rank, lambda1, lambda2, } enable_shift: Use shifted model with shift range (-12,12) enable_weighting: Use weighted model with weights as defined in experiments.simulation::data_weights enable_convolution: Use convolutional model. mlflow_tags: Dict of tags to mark the MLFLow run with. For example { \"Developer\": \"Ola Nordmann\", \"Notes\": \"Using a very slow computer.\", } dataset_path: pathlib.Path The path were dataset is stored. TODO: option to plot and log figures TODO: option to store and log artifacts like U,V,M,datasets,etc TODO: more clearly separate train and predict \"\"\" # Setup and loading # dataset = Dataset . from_file ( dataset_path ) X_train , X_test , M_train , M_test = dataset . get_split_X_M () # Simulate data for a prediction task by selecting the last data point in each # sample vetor as the prediction target X_test_masked , t_pred , x_true = prediction_data ( X_test ) mlflow . start_run () mlflow . set_tags ( mlflow_tags ) mlflow . log_params ( hyperparams ) mlflow . log_params ( dataset . prefixed_metadata ()) # Generate the model model = model_factory ( X_train , shift_range = list ( range ( - 12 , 13 )) if enable_shift else [], use_convolution = enable_convolution , use_weights = enable_weighting , ** hyperparams , ) mlflow . log_param ( \"model_name\" , model . config . get_short_model_name ()) # Training and testing # # Train the model (i.e. perform the matrix completion) extra_metrics = { \"recMSE\" : lambda model : reconstruction_mse ( M_train , X_train , model . M ), } results = model . matrix_completion ( extra_metrics = extra_metrics ) # Predict the risk over the test set p_pred = model . predict_probability ( X_test_masked , t_pred ) # Estimate the mostl likely prediction result from the probabilities x_pred = 1.0 + np . argmax ( p_pred , axis = 1 ) # We set the backend to have the figure show on Mac. # See https://matplotlib.org/stable/users/explain/backends.html for a reference # on the matplotlib backends. try : matplotlib . use ( \"MacOSX\" ) except ImportError : # We are not on a Mac pass plot_certainty ( p_pred , x_true ) # Log some metrics mlflow . log_metric ( \"matthew_score\" , matthews_corrcoef ( x_true , x_pred ), step = results [ \"epochs\" ][ - 1 ] ) mlflow . log_metric ( \"accuracy\" , accuracy_score ( x_true , x_pred ), step = results [ \"epochs\" ][ - 1 ] ) for epoch , loss in zip ( results [ \"epochs\" ], results [ \"loss\" ]): mlflow . log_metric ( \"loss\" , loss , step = epoch ) for metric in extra_metrics : for epoch , metric_value in zip ( results [ \"epochs\" ], results [ metric ]): mlflow . log_metric ( metric , metric_value , step = epoch ) mlflow . log_metric ( \"norm_difference\" , np . linalg . norm ( results [ \"M\" ] - M_train )) # Plotting # if settings . create_path_default : FIGURE_PATH . mkdir ( parents = True , exist_ok = True ) plot_coefs ( results [ \"U\" ], FIGURE_PATH ) plot_basis ( results [ \"V\" ], FIGURE_PATH ) plot_confusion ( x_true , x_pred , FIGURE_PATH ) plot_roc_curve ( x_true , p_pred , FIGURE_PATH ) mlflow . log_artifacts ( FIGURE_PATH ) mlflow . end_run () def main (): # Generate some data Dataset . generate ( N = 1000 , T = 50 , rank = 5 , sparsity_level = 6 ) . save ( DATASET_PATH ) USE_GPU = False if not USE_GPU : tf . config . set_visible_devices ([], \"GPU\" ) mlflow_tags = { \"Developer\" : \"Thorvald M. Ballestad\" , \"GPU\" : USE_GPU , \"Notes\" : \"tf.function commented out\" , } # NB! lamabda1, lambda2, lambda3 does *not* correspond directly to # the notation used in the master thesis. hyperparams = { \"rank\" : 5 , \"lambda1\" : 10 , \"lambda2\" : 10 , \"lambda3\" : 100 , } for shift , weight , convolve in product ([ False , True ], repeat = 3 ): experiment ( hyperparams , shift , weight , convolve , mlflow_tags = mlflow_tags , ) if __name__ == \"__main__\" : main ()","title":"Simple example"},{"location":"examples/example/#simple-example","text":"# example.py import pathlib from itertools import product import matplotlib import mlflow import numpy as np import tensorflow as tf from sklearn.metrics import accuracy_score , matthews_corrcoef from matfact import settings from matfact.data_generation import Dataset from matfact.model import model_factory , prediction_data , reconstruction_mse from matfact.plotting import ( plot_basis , plot_certainty , plot_coefs , plot_confusion , plot_roc_curve , ) from matfact.settings import DATASET_PATH , FIGURE_PATH def experiment ( hyperparams , enable_shift : bool = False , enable_weighting : bool = False , enable_convolution : bool = False , mlflow_tags : dict = None , dataset_path : pathlib . Path = DATASET_PATH , ): \"\"\"Execute and log an experiment. Loads the dataset in dataset_path and splits this into train and test sets. The test set is masked, so that the last observation is hidden. The matrix completion is solved on the train set and then the probability of the possible states of the (masked) train set is comptued. The run is tracked using MLFLow, using several metrics and artifacts (files). Parameters: hyperparams: Dict of hyperparameters passed to the model. Common for all models is { rank, lambda1, lambda2, } enable_shift: Use shifted model with shift range (-12,12) enable_weighting: Use weighted model with weights as defined in experiments.simulation::data_weights enable_convolution: Use convolutional model. mlflow_tags: Dict of tags to mark the MLFLow run with. For example { \"Developer\": \"Ola Nordmann\", \"Notes\": \"Using a very slow computer.\", } dataset_path: pathlib.Path The path were dataset is stored. TODO: option to plot and log figures TODO: option to store and log artifacts like U,V,M,datasets,etc TODO: more clearly separate train and predict \"\"\" # Setup and loading # dataset = Dataset . from_file ( dataset_path ) X_train , X_test , M_train , M_test = dataset . get_split_X_M () # Simulate data for a prediction task by selecting the last data point in each # sample vetor as the prediction target X_test_masked , t_pred , x_true = prediction_data ( X_test ) mlflow . start_run () mlflow . set_tags ( mlflow_tags ) mlflow . log_params ( hyperparams ) mlflow . log_params ( dataset . prefixed_metadata ()) # Generate the model model = model_factory ( X_train , shift_range = list ( range ( - 12 , 13 )) if enable_shift else [], use_convolution = enable_convolution , use_weights = enable_weighting , ** hyperparams , ) mlflow . log_param ( \"model_name\" , model . config . get_short_model_name ()) # Training and testing # # Train the model (i.e. perform the matrix completion) extra_metrics = { \"recMSE\" : lambda model : reconstruction_mse ( M_train , X_train , model . M ), } results = model . matrix_completion ( extra_metrics = extra_metrics ) # Predict the risk over the test set p_pred = model . predict_probability ( X_test_masked , t_pred ) # Estimate the mostl likely prediction result from the probabilities x_pred = 1.0 + np . argmax ( p_pred , axis = 1 ) # We set the backend to have the figure show on Mac. # See https://matplotlib.org/stable/users/explain/backends.html for a reference # on the matplotlib backends. try : matplotlib . use ( \"MacOSX\" ) except ImportError : # We are not on a Mac pass plot_certainty ( p_pred , x_true ) # Log some metrics mlflow . log_metric ( \"matthew_score\" , matthews_corrcoef ( x_true , x_pred ), step = results [ \"epochs\" ][ - 1 ] ) mlflow . log_metric ( \"accuracy\" , accuracy_score ( x_true , x_pred ), step = results [ \"epochs\" ][ - 1 ] ) for epoch , loss in zip ( results [ \"epochs\" ], results [ \"loss\" ]): mlflow . log_metric ( \"loss\" , loss , step = epoch ) for metric in extra_metrics : for epoch , metric_value in zip ( results [ \"epochs\" ], results [ metric ]): mlflow . log_metric ( metric , metric_value , step = epoch ) mlflow . log_metric ( \"norm_difference\" , np . linalg . norm ( results [ \"M\" ] - M_train )) # Plotting # if settings . create_path_default : FIGURE_PATH . mkdir ( parents = True , exist_ok = True ) plot_coefs ( results [ \"U\" ], FIGURE_PATH ) plot_basis ( results [ \"V\" ], FIGURE_PATH ) plot_confusion ( x_true , x_pred , FIGURE_PATH ) plot_roc_curve ( x_true , p_pred , FIGURE_PATH ) mlflow . log_artifacts ( FIGURE_PATH ) mlflow . end_run () def main (): # Generate some data Dataset . generate ( N = 1000 , T = 50 , rank = 5 , sparsity_level = 6 ) . save ( DATASET_PATH ) USE_GPU = False if not USE_GPU : tf . config . set_visible_devices ([], \"GPU\" ) mlflow_tags = { \"Developer\" : \"Thorvald M. Ballestad\" , \"GPU\" : USE_GPU , \"Notes\" : \"tf.function commented out\" , } # NB! lamabda1, lambda2, lambda3 does *not* correspond directly to # the notation used in the master thesis. hyperparams = { \"rank\" : 5 , \"lambda1\" : 10 , \"lambda2\" : 10 , \"lambda3\" : 100 , } for shift , weight , convolve in product ([ False , True ], repeat = 3 ): experiment ( hyperparams , shift , weight , convolve , mlflow_tags = mlflow_tags , ) if __name__ == \"__main__\" : main ()","title":"Simple example"},{"location":"examples/example_class_based/","text":"Example usage of class based Matfact API. Simple demo of the MatFact class. # example_class_based.py import contextlib import tensorflow as tf from matfact import settings from matfact.data_generation.dataset import Dataset from matfact.model.config import ModelConfig from matfact.model.matfact import ArgmaxPredictor , MatFact from matfact.model.predict.dataset_utils import prediction_data # Disabling the GPU makes everything faster. # If tf is already initialized, we cannot modified visible devices, in which # case we just proceed. with contextlib . suppress ( RuntimeError ): tf . config . set_visible_devices ([], \"GPU\" ) def main (): dataset = Dataset . from_file ( settings . DATASET_PATH ) X_train , X_test , * _ = dataset . get_split_X_M () matfact = MatFact ( ModelConfig (), predictor = ArgmaxPredictor ()) matfact . fit ( X_train ) X_test_masked , time_steps , true_states = prediction_data ( X_test ) print ( matfact . predict_probabilities ( X_test_masked , time_steps )) print ( true_states ) print ( matfact . predict ( X_test_masked , time_steps )) if __name__ == \"__main__\" : main ()","title":"Example usage of class based Matfact API."},{"location":"examples/example_class_based/#example-usage-of-class-based-matfact-api","text":"Simple demo of the MatFact class. # example_class_based.py import contextlib import tensorflow as tf from matfact import settings from matfact.data_generation.dataset import Dataset from matfact.model.config import ModelConfig from matfact.model.matfact import ArgmaxPredictor , MatFact from matfact.model.predict.dataset_utils import prediction_data # Disabling the GPU makes everything faster. # If tf is already initialized, we cannot modified visible devices, in which # case we just proceed. with contextlib . suppress ( RuntimeError ): tf . config . set_visible_devices ([], \"GPU\" ) def main (): dataset = Dataset . from_file ( settings . DATASET_PATH ) X_train , X_test , * _ = dataset . get_split_X_M () matfact = MatFact ( ModelConfig (), predictor = ArgmaxPredictor ()) matfact . fit ( X_train ) X_test_masked , time_steps , true_states = prediction_data ( X_test ) print ( matfact . predict_probabilities ( X_test_masked , time_steps )) print ( true_states ) print ( matfact . predict ( X_test_masked , time_steps )) if __name__ == \"__main__\" : main ()","title":"Example usage of class based Matfact API."},{"location":"examples/example_hyperparamsearch/","text":"Example of an hyperparameter search. # example_hyperparamsearch.py import functools from typing import Callable import mlflow import numpy as np import tensorflow as tf from sklearn.model_selection import KFold from skopt import gp_minimize from skopt.space import Real from skopt.utils import use_named_args from matfact.data_generation import Dataset from matfact.model import train_and_log from matfact.model.logging import MLFlowBatchLogger , MLFlowLogger , dummy_logger_context from matfact.settings import BASE_PATH , DATASET_PATH def get_objective ( data : Dataset , search_space : list , ** hyperparams ): \"\"\"Simple train-test based search.\"\"\" X_train , X_test , * _ = data . get_split_X_M () @use_named_args ( search_space ) def objective ( ** search_hyperparams ): hyperparams . update ( search_hyperparams ) mlflow_output = train_and_log ( X_train , X_test , logger_context = MLFlowLogger (), dict_to_log = data . prefixed_metadata (), log_loss = False , ** hyperparams , ) # The score logged, the Matthew correlation coefficient, is 'higher is # better', while we are minimizing. return - mlflow_output [ \"metrics\" ][ \"matthew_score\" ] return objective def get_objective_CV ( data : Dataset , search_space : list , n_splits : int = 5 , log_folds : bool = False , ** hyperparams , ): \"\"\"Cross validation search.\"\"\" kf = KFold ( n_splits = n_splits ) X , _ = data . get_X_M () logger_context = MLFlowLogger () if log_folds else dummy_logger_context @use_named_args ( search_space ) def objective ( ** search_hyperparams ): hyperparams . update ( search_hyperparams ) scores = [] with MLFlowBatchLogger () as logger : for train_idx , test_idx in kf . split ( X ): mlflow_output = train_and_log ( X [ train_idx ], X [ test_idx ], dict_to_log = data . prefixed_metadata (), logger_context = logger_context , log_loss = False , ** hyperparams , ) # The score logged, the Matthew correlation coefficient, is 'higher is # better', while we are minimizing. logger ( mlflow_output ) scores . append ( - mlflow_output [ \"metrics\" ][ \"matthew_score\" ]) return np . mean ( scores ) return objective def example_hyperparameter_search ( objective_getter : Callable = get_objective_CV ): \"\"\"Example implementation of hyperparameter search. objective_getter: callable returning an objective function.\"\"\" tf . config . set_visible_devices ([], \"GPU\" ) mlflow . set_tracking_uri ( BASE_PATH / \"mlruns\" ) space = ( Real ( - 5.0 , 1 , name = \"lambda1\" ), Real ( 8 , 20 , name = \"lambda2\" ), Real ( 0.0 , 20 , name = \"lambda3\" ), ) # Load data try : data = Dataset . from_file ( DATASET_PATH ) except FileNotFoundError : # No data loaded data = Dataset . generate ( 1000 , 40 , 5 , 5 ) with mlflow . start_run (): res_gp = gp_minimize ( objective_getter ( data , search_space = space , use_convolution = False , shift_range = None ), space , n_calls = 10 , ) best_values = res_gp [ \"x\" ] best_score = res_gp [ \"fun\" ] # We are minimizing, so the best_score is inverted. mlflow . log_metric ( \"best_score\" , - best_score ) for param , value in zip ( space , best_values ): mlflow . log_param ( f \"best_ { param . name } \" , value ) mlflow . set_tag ( \"Notes\" , \"Hyperparameter search\" ) def main (): # Set objective_getter to get_objective_CV to use cross validation. # Otherwise, get_objective uses simple train/test split. example_hyperparameter_search ( objective_getter = functools . partial ( get_objective_CV , log_folds = True ) ) if __name__ == \"__main__\" : main ()","title":"Example of an hyperparameter search."},{"location":"examples/example_hyperparamsearch/#example-of-an-hyperparameter-search","text":"# example_hyperparamsearch.py import functools from typing import Callable import mlflow import numpy as np import tensorflow as tf from sklearn.model_selection import KFold from skopt import gp_minimize from skopt.space import Real from skopt.utils import use_named_args from matfact.data_generation import Dataset from matfact.model import train_and_log from matfact.model.logging import MLFlowBatchLogger , MLFlowLogger , dummy_logger_context from matfact.settings import BASE_PATH , DATASET_PATH def get_objective ( data : Dataset , search_space : list , ** hyperparams ): \"\"\"Simple train-test based search.\"\"\" X_train , X_test , * _ = data . get_split_X_M () @use_named_args ( search_space ) def objective ( ** search_hyperparams ): hyperparams . update ( search_hyperparams ) mlflow_output = train_and_log ( X_train , X_test , logger_context = MLFlowLogger (), dict_to_log = data . prefixed_metadata (), log_loss = False , ** hyperparams , ) # The score logged, the Matthew correlation coefficient, is 'higher is # better', while we are minimizing. return - mlflow_output [ \"metrics\" ][ \"matthew_score\" ] return objective def get_objective_CV ( data : Dataset , search_space : list , n_splits : int = 5 , log_folds : bool = False , ** hyperparams , ): \"\"\"Cross validation search.\"\"\" kf = KFold ( n_splits = n_splits ) X , _ = data . get_X_M () logger_context = MLFlowLogger () if log_folds else dummy_logger_context @use_named_args ( search_space ) def objective ( ** search_hyperparams ): hyperparams . update ( search_hyperparams ) scores = [] with MLFlowBatchLogger () as logger : for train_idx , test_idx in kf . split ( X ): mlflow_output = train_and_log ( X [ train_idx ], X [ test_idx ], dict_to_log = data . prefixed_metadata (), logger_context = logger_context , log_loss = False , ** hyperparams , ) # The score logged, the Matthew correlation coefficient, is 'higher is # better', while we are minimizing. logger ( mlflow_output ) scores . append ( - mlflow_output [ \"metrics\" ][ \"matthew_score\" ]) return np . mean ( scores ) return objective def example_hyperparameter_search ( objective_getter : Callable = get_objective_CV ): \"\"\"Example implementation of hyperparameter search. objective_getter: callable returning an objective function.\"\"\" tf . config . set_visible_devices ([], \"GPU\" ) mlflow . set_tracking_uri ( BASE_PATH / \"mlruns\" ) space = ( Real ( - 5.0 , 1 , name = \"lambda1\" ), Real ( 8 , 20 , name = \"lambda2\" ), Real ( 0.0 , 20 , name = \"lambda3\" ), ) # Load data try : data = Dataset . from_file ( DATASET_PATH ) except FileNotFoundError : # No data loaded data = Dataset . generate ( 1000 , 40 , 5 , 5 ) with mlflow . start_run (): res_gp = gp_minimize ( objective_getter ( data , search_space = space , use_convolution = False , shift_range = None ), space , n_calls = 10 , ) best_values = res_gp [ \"x\" ] best_score = res_gp [ \"fun\" ] # We are minimizing, so the best_score is inverted. mlflow . log_metric ( \"best_score\" , - best_score ) for param , value in zip ( space , best_values ): mlflow . log_param ( f \"best_ { param . name } \" , value ) mlflow . set_tag ( \"Notes\" , \"Hyperparameter search\" ) def main (): # Set objective_getter to get_objective_CV to use cross validation. # Otherwise, get_objective uses simple train/test split. example_hyperparameter_search ( objective_getter = functools . partial ( get_objective_CV , log_folds = True ) ) if __name__ == \"__main__\" : main ()","title":"Example of an hyperparameter search."},{"location":"examples/example_train_and_log/","text":"Example using train_and_log for simpler workflow. Warning We might change the usage of train_and_log in the future. # example_train_and_log.py import pathlib from itertools import product import tensorflow as tf from matfact.data_generation import Dataset from matfact.model import reconstruction_mse , train_and_log from matfact.model.logging import MLFlowLoggerDiagnostic from matfact.settings import DATASET_PATH , FIGURE_PATH def experiment ( hyperparams , enable_shift : bool = False , enable_weighting : bool = False , enable_convolution : bool = False , mlflow_tags : dict | None = None , dataset_path : pathlib . Path = DATASET_PATH , ): \"\"\"Execute and log an experiment. Loads the dataset in dataset_path and splits this into train and test sets. The test set is masked, so that the last observation is hidden. The matrix completion is solved on the train set and then the probability of the possible states of the (masked) train set is comptued. The run is tracked using MLFLow, using several metrics and artifacts (files). Parameters: hyperparams: Dict of hyperparameters passed to the model. Common for all models is { rank, lambda1, lambda2, } enable_shift: Use shifted model with shift range (-12,12) enable_weighting: Use weighted model with weights as defined in experiments.simulation::data_weights enable_convolution: Use convolutional model. mlflow_tags: Dict of tags to mark the MLFLow run with. For example { \"Developer\": \"Ola Nordmann\", \"Notes\": \"Using a very slow computer.\", } dataset_path: pathlib.Path The path were dataset is stored. \"\"\" # Setup and loading # dataset = Dataset . from_file ( dataset_path ) X_train , X_test , M_train , _ = dataset . get_split_X_M () shift_range = list ( range ( - 12 , 13 )) if enable_shift else [] extra_metrics = { \"recMSE\" : lambda model : reconstruction_mse ( M_train , model . X , model . M ), } train_and_log ( X_train , X_test , shift_range = shift_range , use_weights = enable_weighting , extra_metrics = extra_metrics , use_convolution = enable_convolution , logger_context = MLFlowLoggerDiagnostic ( FIGURE_PATH , extra_tags = mlflow_tags ), ** hyperparams ) def main (): # Generate some data Dataset . generate ( N = 1000 , T = 50 , rank = 5 , sparsity_level = 6 ) . save ( DATASET_PATH ) USE_GPU = False if not USE_GPU : tf . config . set_visible_devices ([], \"GPU\" ) mlflow_tags = { \"Developer\" : \"Thorvald M. Ballestad\" , \"GPU\" : USE_GPU , \"Notes\" : \"tf.function commented out\" , } # NB! lamabda1, lambda2, lambda3 does *not* correspond directly to # the notation used in the master thesis. hyperparams = { \"rank\" : 5 , \"lambda1\" : - 0.021857774198331015 , \"lambda2\" : 8 , \"lambda3\" : 4.535681885641427 , } for shift , weight , convolve in product ([ False , True ], repeat = 3 ): experiment ( hyperparams , shift , weight , convolve , mlflow_tags = mlflow_tags , ) if __name__ == \"__main__\" : main ()","title":"Example using `train_and_log` for simpler workflow."},{"location":"examples/example_train_and_log/#example-using-train_and_log-for-simpler-workflow","text":"Warning We might change the usage of train_and_log in the future. # example_train_and_log.py import pathlib from itertools import product import tensorflow as tf from matfact.data_generation import Dataset from matfact.model import reconstruction_mse , train_and_log from matfact.model.logging import MLFlowLoggerDiagnostic from matfact.settings import DATASET_PATH , FIGURE_PATH def experiment ( hyperparams , enable_shift : bool = False , enable_weighting : bool = False , enable_convolution : bool = False , mlflow_tags : dict | None = None , dataset_path : pathlib . Path = DATASET_PATH , ): \"\"\"Execute and log an experiment. Loads the dataset in dataset_path and splits this into train and test sets. The test set is masked, so that the last observation is hidden. The matrix completion is solved on the train set and then the probability of the possible states of the (masked) train set is comptued. The run is tracked using MLFLow, using several metrics and artifacts (files). Parameters: hyperparams: Dict of hyperparameters passed to the model. Common for all models is { rank, lambda1, lambda2, } enable_shift: Use shifted model with shift range (-12,12) enable_weighting: Use weighted model with weights as defined in experiments.simulation::data_weights enable_convolution: Use convolutional model. mlflow_tags: Dict of tags to mark the MLFLow run with. For example { \"Developer\": \"Ola Nordmann\", \"Notes\": \"Using a very slow computer.\", } dataset_path: pathlib.Path The path were dataset is stored. \"\"\" # Setup and loading # dataset = Dataset . from_file ( dataset_path ) X_train , X_test , M_train , _ = dataset . get_split_X_M () shift_range = list ( range ( - 12 , 13 )) if enable_shift else [] extra_metrics = { \"recMSE\" : lambda model : reconstruction_mse ( M_train , model . X , model . M ), } train_and_log ( X_train , X_test , shift_range = shift_range , use_weights = enable_weighting , extra_metrics = extra_metrics , use_convolution = enable_convolution , logger_context = MLFlowLoggerDiagnostic ( FIGURE_PATH , extra_tags = mlflow_tags ), ** hyperparams ) def main (): # Generate some data Dataset . generate ( N = 1000 , T = 50 , rank = 5 , sparsity_level = 6 ) . save ( DATASET_PATH ) USE_GPU = False if not USE_GPU : tf . config . set_visible_devices ([], \"GPU\" ) mlflow_tags = { \"Developer\" : \"Thorvald M. Ballestad\" , \"GPU\" : USE_GPU , \"Notes\" : \"tf.function commented out\" , } # NB! lamabda1, lambda2, lambda3 does *not* correspond directly to # the notation used in the master thesis. hyperparams = { \"rank\" : 5 , \"lambda1\" : - 0.021857774198331015 , \"lambda2\" : 8 , \"lambda3\" : 4.535681885641427 , } for shift , weight , convolve in product ([ False , True ], repeat = 3 ): experiment ( hyperparams , shift , weight , convolve , mlflow_tags = mlflow_tags , ) if __name__ == \"__main__\" : main ()","title":"Example using train_and_log for simpler workflow."}]}